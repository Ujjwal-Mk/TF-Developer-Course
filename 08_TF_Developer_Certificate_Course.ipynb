{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rj7sBkxRK6G",
        "outputId": "1e663cc2-0b3a-43f0-d474-9f7e1e112a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "2.14.0-dev20230730\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67y0phrRQnz",
        "outputId": "81d04c0b-1f69-419f-f579-b8cd75387b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 'helper_functions.py' already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "  !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "  print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n",
        "\n",
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5L-H4otpgvS"
      },
      "source": [
        "We will be using RNN to train textual information\n",
        "\n",
        "**Basic Structure of RNN**\n",
        "* Input layer\n",
        "* text_vectorizer(input)\n",
        "* embedding(x)\n",
        "* LSTM -> activation = tanh\n",
        "* output_Dense -> activation = sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0eqNb9Nsyzi"
      },
      "source": [
        "### Get the text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Fp9BY2pkCZ",
        "outputId": "68fbdfce-f7fa-4d30-89d2-7ee2bfeeac45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
        "    !wget https://storage.googlqeapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "    unzip_data(\"nlp_getting_started.zip\")\n",
        "else:\n",
        "  print(\"[INFO] already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzD1DRKtRlJ"
      },
      "source": [
        "### Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qyWmNLXTtC_x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BoTo82yJt4no",
        "outputId": "913f97ff-16a7-4f96-c138-4b2126474a9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5ImBTHVft6K2",
        "outputId": "0b19d104-d524-4206-9c13-486aa55e5c34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>7934</td>\n",
              "      <td>rainstorm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@Calum5SOS you look like you got caught in a r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>2538</td>\n",
              "      <td>collision</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my favorite lady came to our volunteer meeting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>2611</td>\n",
              "      <td>crashed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@brianroemmele UX fail of EMV - people want to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6810</th>\n",
              "      <td>9756</td>\n",
              "      <td>tragedy</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Can't find my ariana grande shirt  this is a f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>6254</td>\n",
              "      <td>hijacking</td>\n",
              "      <td>Athens,Greece</td>\n",
              "      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                    NaN   \n",
              "2227  3185       deluge                    NaN   \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                    NaN   \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "5559  7934    rainstorm                    NaN   \n",
              "1765  2538    collision                    NaN   \n",
              "1817  2611      crashed                    NaN   \n",
              "6810  9756      tragedy        Los Angeles, CA   \n",
              "4398  6254    hijacking          Athens,Greece   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "5559  @Calum5SOS you look like you got caught in a r...       0  \n",
              "1765  my favorite lady came to our volunteer meeting...       1  \n",
              "1817  @brianroemmele UX fail of EMV - people want to...       1  \n",
              "6810  Can't find my ariana grande shirt  this is a f...       0  \n",
              "4398  The Murderous Story Of AmericaÛªs First Hijac...       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Shuffle to train data\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "train_df_shuffled.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2wbB2XBBuUsd",
        "outputId": "5d7aa573-919e-4194-a637-848374c73253"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're shaking...It's an earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They'd probably still show more life than Arse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey! How are you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a nice hat?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fuck off!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
              "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
              "7  22     NaN      NaN                                  Hey! How are you?\n",
              "8  27     NaN      NaN                                   What a nice hat?\n",
              "9  29     NaN      NaN                                          Fuck off!"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_data\n",
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNWm50cubJ8",
        "outputId": "660bd2ae-b113-448f-d7fd-7a4a79e2cd2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#How many examples in each class\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA79OdN4umTy",
        "outputId": "d1f9094c-38b4-494b-e75f-d7ee32698c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6363\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "Demolition Means Progress: Flint Michigan and the Fate of the American Metropolis Highsmith https://t.co/ZvoBMDxHGP\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "Madhya Pradesh Train Derailment: Village Youth Saved Many Lives\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "500 deaths a year from foodborne illness... @frackfreelancs dears... @DECCgovuk @frackfree_eu @tarleton_sophie http://t.co/JSccX8k0jA\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "namjoon's FANTASTIC IS BOMB BYE OMG\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "@JulieChen she shouldn't. Being with them is gonna ruin her game and Vanessa is a great player\n",
            "\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# visualise some random training examples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "print(random_index)\n",
        "for row in train_df_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  index,text,target = row\n",
        "  print(f\"Target: {target}, {'(real disaster)' if target > 0 else '(not a real disaster)'}\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRuczOy1Hze",
        "outputId": "7fd6c85e-4713-4d69-e837-61fd589df245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_df_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ilKeV5jwa0E"
      },
      "outputs": [],
      "source": [
        "#Splitting the data to create validation -> (train + validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2LtkKK08Rd",
        "outputId": "7109197e-f115-4f06-8be1-d9913d133cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check length\n",
        "len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZ7B-jT1EHj",
        "outputId": "66494cf4-27da-4c56-e9ee-5a9272a8b1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object),\n",
              " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check few sample\n",
        "train_sentences[:10],train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfc38t53GjO"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "1. tokenization -> can get too big as the number of text sample increases\n",
        "2. Embedding -> richer representation of relationship be between tokens, deeper the realtion between words, more the number/embedding for that token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yru6-5TJ1U17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                                    split=\"whitespace\", # how to split tokens\n",
        "                                                    ngrams=None, # create groups of n-words?\n",
        "                                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_length,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Don't mess with my Daddy I can be a massacre. #BeCarefulHarry      \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[  63, 2389,   14,   13,    1,    8,   71,   21,    3,  344,    1,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
          ]
        }
      ],
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.layers.core.embedding.Embedding at 0x7f7b521a16a0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\") \n",
        "\n",
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Toronto going crazy for the blue jays. Can you imagine if the leafs get good? The city might literally explode.\n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.04486163,  0.02573014, -0.02279862, ...,  0.03354745,\n",
              "          0.02301952,  0.0467402 ],\n",
              "        [ 0.00861071,  0.04662026, -0.03669335, ..., -0.04445063,\n",
              "         -0.0062235 ,  0.0347099 ],\n",
              "        [-0.01213051,  0.01153048,  0.03086859, ...,  0.04621183,\n",
              "         -0.00443321, -0.00083055],\n",
              "        ...,\n",
              "        [ 0.00433008, -0.02822622, -0.01093443, ...,  0.01338464,\n",
              "          0.02854312, -0.03799286],\n",
              "        [ 0.00428232, -0.01403551,  0.03349373, ..., -0.01021522,\n",
              "          0.04346382,  0.04410707],\n",
              "        [ 0.03060034, -0.00804458,  0.01401711, ..., -0.02390458,\n",
              "         -0.0151305 ,  0.01741089]]], dtype=float32)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([-0.04486163,  0.02573014, -0.02279862,  0.03093452,  0.03159383,\n",
              "       -0.03235666,  0.01509494,  0.00938249,  0.00549501,  0.02309141,\n",
              "        0.01819383, -0.00858293, -0.02414702,  0.03628812,  0.03950496,\n",
              "        0.00044477,  0.0281427 ,  0.04461784, -0.04495106, -0.04993987,\n",
              "        0.04470675, -0.04416573, -0.00659142,  0.00944578, -0.03977437,\n",
              "        0.03070751, -0.01258197,  0.02826376,  0.02297081, -0.02892262,\n",
              "       -0.03113362,  0.02961594,  0.0044076 , -0.0210196 ,  0.0055593 ,\n",
              "       -0.0133971 , -0.03116485,  0.02102199,  0.02906496,  0.01570168,\n",
              "        0.00256414, -0.02553383, -0.03726934,  0.02964776,  0.00096213,\n",
              "        0.02992559,  0.01166254,  0.01386534,  0.0266539 , -0.01712762,\n",
              "        0.04040455, -0.00242213, -0.0348905 ,  0.03203893, -0.00558251,\n",
              "       -0.02199773, -0.03389901, -0.03932511,  0.01213367,  0.03621433,\n",
              "       -0.01713515, -0.03027561,  0.01116436, -0.03695067, -0.04738715,\n",
              "        0.01366898,  0.00203271, -0.01139911,  0.0019037 , -0.01931064,\n",
              "       -0.02284697,  0.01649609,  0.00391136, -0.02525981,  0.02096262,\n",
              "        0.01934625,  0.02507793, -0.00243653, -0.01962911,  0.01383067,\n",
              "       -0.00557101,  0.0090435 , -0.01769236, -0.00724319,  0.04907865,\n",
              "       -0.02547189,  0.01895043, -0.02225183,  0.01591852, -0.01828222,\n",
              "        0.02993316,  0.01795201, -0.03125449, -0.01101277, -0.01837828,\n",
              "        0.0359758 , -0.03570974, -0.01375473,  0.01016482,  0.02313867,\n",
              "        0.03722996,  0.00601375, -0.02134964, -0.04907857,  0.01059958,\n",
              "       -0.04458005, -0.0137164 ,  0.00852137, -0.01537541, -0.02653297,\n",
              "       -0.03123584,  0.00851602,  0.01499606,  0.04493102,  0.01674799,\n",
              "       -0.00545947,  0.03165771, -0.03282825, -0.03541698, -0.02209922,\n",
              "        0.0042323 , -0.02207905,  0.03911865,  0.04521007, -0.03987895,\n",
              "        0.03354745,  0.02301952,  0.0467402 ], dtype=float32)>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelling a text dataset\n",
        "More specifically, we'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model 0\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our baseline model achieves an accuracy of: 79.27%\n"
          ]
        }
      ],
      "source": [
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_functions import calculate_results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 1: A simple dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1280129 (4.88 MB)\n",
            "Trainable params: 1280129 (4.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create directory to save TensorBoard logs\n",
        "SAVE_DIR = \"model_logs\"\n",
        "\n",
        "# Build model with the Functional API\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into numbers\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
        "# Compile model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/simple_dense_model/20230817-221939\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 19ms/step - loss: 0.6090 - accuracy: 0.6944 - val_loss: 0.5354 - val_accuracy: 0.7546\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.4406 - accuracy: 0.8192 - val_loss: 0.4690 - val_accuracy: 0.7861\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.3459 - accuracy: 0.8615 - val_loss: 0.4589 - val_accuracy: 0.7887\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2844 - accuracy: 0.8923 - val_loss: 0.4640 - val_accuracy: 0.7900\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2375 - accuracy: 0.9115 - val_loss: 0.4767 - val_accuracy: 0.7861\n"
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
        "                                                                     experiment_name=\"simple_dense_model\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.7861\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.47670021653175354, 0.7860892415046692]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 972us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.3493701 ],\n",
              "       [0.5704087 ],\n",
              "       [0.9875553 ],\n",
              "       [0.14310086],\n",
              "       [0.02676124],\n",
              "       [0.8892967 ],\n",
              "       [0.58423054],\n",
              "       [0.99571854],\n",
              "       [0.9833093 ],\n",
              "       [0.36435312]], dtype=float32)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_preds=tf.squeeze(tf.round(model_1_pred_probs))\n",
        "model_1_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 77.03412073490814,\n",
              " 'precision': 0.7743290047665735,\n",
              " 'recall': 0.7703412073490814,\n",
              " 'f1': 0.7671895343790688}"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_results=calculate_results(val_labels,model_1_preds)\n",
        "model_1_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recurrent Neural Network\n",
        "* RNN's are useful for sequence data\n",
        "* The premise of a recurrent neural network is to use the representation of previous input to aid the representation of a later input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 : LSTM -> Long short term memory\n",
        "\n",
        "Input(text) -> Toeknize -> Embedding -> Layers(RNN/Dense) -> Output(label probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 15, 128)\n",
            "(None, 64)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 15, 64)            49408     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 15, 64)            33024     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1395521 (5.32 MB)\n",
            "Trainable params: 1395521 (5.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Set random seed and create embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_2_embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.LSTM(64)(x) # return vector for whole sequence\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20230817-221948\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 24ms/step - loss: 0.5074 - accuracy: 0.7500 - val_loss: 0.4586 - val_accuracy: 0.7822\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.3265 - accuracy: 0.8683 - val_loss: 0.5129 - val_accuracy: 0.7782\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.2283 - accuracy: 0.9148 - val_loss: 0.5562 - val_accuracy: 0.7664\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1687 - accuracy: 0.9426 - val_loss: 0.5890 - val_accuracy: 0.7690\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1178 - accuracy: 0.9553 - val_loss: 0.8546 - val_accuracy: 0.7559\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "model_2_history = model_2.fit(train_sentences, train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
        "                                                                     \"LSTM\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.00624254],\n",
              "       [0.6781507 ],\n",
              "       [0.99850875],\n",
              "       [0.12505932],\n",
              "       [0.00421613],\n",
              "       [0.9991742 ],\n",
              "       [0.95853615],\n",
              "       [0.9992612 ],\n",
              "       [0.99921453],\n",
              "       [0.07090925]], dtype=float32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert preds to laels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 75.59055118110236,\n",
              " 'precision': 0.7565294326194807,\n",
              " 'recall': 0.7559055118110236,\n",
              " 'f1': 0.7540879036461907}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_results = calculate_results(val_labels, model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 3: GRU\n",
        "\n",
        "1. Gated recurrent unit is effective and popular\n",
        "2. GRU cell has similar features to LSTM cell but has less paramenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 19ms/step - loss: 0.2323 - accuracy: 0.9057 - val_loss: 0.5319 - val_accuracy: 0.7756\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1598 - accuracy: 0.9423 - val_loss: 0.5956 - val_accuracy: 0.7782\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1281 - accuracy: 0.9534 - val_loss: 0.7568 - val_accuracy: 0.7848\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1092 - accuracy: 0.9606 - val_loss: 0.7570 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0920 - accuracy: 0.9660 - val_loss: 0.7342 - val_accuracy: 0.7874\n"
          ]
        }
      ],
      "source": [
        "# Build GRU RNN\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "# x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.GRU(64)(x)\n",
        "# x = tf.keras.layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                        train_labels,\n",
        "                        epochs=5,\n",
        "                        validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[6.2093776e-02],\n",
              "       [8.3167213e-01],\n",
              "       [9.9965918e-01],\n",
              "       [3.9496008e-02],\n",
              "       [8.4988755e-04],\n",
              "       [9.8857903e-01],\n",
              "       [6.5730685e-01],\n",
              "       [9.9978799e-01],\n",
              "       [9.9955243e-01],\n",
              "       [6.7678499e-01]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert model 3 pred_probs to labels\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.74015748031496,\n",
              " 'precision': 0.7894561193095897,\n",
              " 'recall': 0.7874015748031497,\n",
              " 'f1': 0.7854735566961013}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_results = calculate_results(val_labels,model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model4 - Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 23ms/step - loss: 0.5055 - accuracy: 0.7520 - val_loss: 0.4536 - val_accuracy: 0.7835\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.3130 - accuracy: 0.8740 - val_loss: 0.5152 - val_accuracy: 0.7703\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.2088 - accuracy: 0.9216 - val_loss: 0.5610 - val_accuracy: 0.7572\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1425 - accuracy: 0.9540 - val_loss: 0.6852 - val_accuracy: 0.7612\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.0996 - accuracy: 0.9648 - val_loss: 0.7200 - val_accuracy: 0.7625\n"
          ]
        }
      ],
      "source": [
        "# Trying out on my own\n",
        "text_vectorizer_exp = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                      output_sequence_length=15)\n",
        "text_vectorizer_exp.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer_exp(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4_exp = tf.keras.Model(inputs,outputs)\n",
        "model_4_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_4_history_exp = model_4_exp.fit(train_sentences,\n",
        "                                      train_labels,\n",
        "                                      validation_data=(val_sentences,val_labels),\n",
        "                                      epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normal RNN's go from left to right\n",
        "\n",
        "Bi-directional go from right to left as well as left to right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 6s 20ms/step - loss: 0.5088 - accuracy: 0.7454 - val_loss: 0.4581 - val_accuracy: 0.7795\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3113 - accuracy: 0.8714 - val_loss: 0.5272 - val_accuracy: 0.7756\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.2092 - accuracy: 0.9204 - val_loss: 0.5773 - val_accuracy: 0.7690\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1428 - accuracy: 0.9520 - val_loss: 0.6659 - val_accuracy: 0.7730\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1006 - accuracy: 0.9660 - val_loss: 0.6933 - val_accuracy: 0.7677\n"
          ]
        }
      ],
      "source": [
        "# model 4\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                                    output_sequence_length=15)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_4_history = model_4.fit(train_sentences,train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.02518224],\n",
              "       [0.8288253 ],\n",
              "       [0.9994019 ],\n",
              "       [0.14624396],\n",
              "       [0.00539156],\n",
              "       [0.99798965],\n",
              "       [0.88434756],\n",
              "       [0.9995727 ],\n",
              "       [0.99956816],\n",
              "       [0.21081477]], dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.77165354330708,\n",
              " 'precision': 0.7679905783589133,\n",
              " 'recall': 0.7677165354330708,\n",
              " 'f1': 0.7663871505080737}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_results = calculate_results(val_labels,model_4_preds)\n",
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
        "conv_1d = tf.keras.layers.Conv1D(32,\n",
        "                                 5,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding=\"valid\")\n",
        "conv_1d_output = conv_1d(embedding_test)\n",
        "\n",
        "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output)\n",
        "\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.02745367,  0.04983198, -0.02139239, ...,  0.03518584,\n",
              "          0.01700259, -0.0483982 ],\n",
              "        [-0.05949971,  0.04479698, -0.02952983, ..., -0.06937435,\n",
              "          0.02570894,  0.02816616],\n",
              "        [-0.05402016, -0.01759368, -0.02602993, ...,  0.03084477,\n",
              "         -0.02211493,  0.02448285],\n",
              "        ...,\n",
              "        [ 0.01937979,  0.01162951, -0.00827661, ..., -0.00865137,\n",
              "          0.0256858 , -0.02262224],\n",
              "        [ 0.01937979,  0.01162951, -0.00827661, ..., -0.00865137,\n",
              "          0.0256858 , -0.02262224],\n",
              "        [ 0.01937979,  0.01162951, -0.00827661, ..., -0.00865137,\n",
              "          0.0256858 , -0.02262224]]], dtype=float32)>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
              "array([[[0.10896946, 0.        , 0.02907739, 0.05390075, 0.02455719,\n",
              "         0.        , 0.01336014, 0.04423492, 0.03102152, 0.        ,\n",
              "         0.0317586 , 0.        , 0.        , 0.02333548, 0.        ,\n",
              "         0.00820252, 0.        , 0.00179943, 0.06839722, 0.        ,\n",
              "         0.        , 0.04786102, 0.00911995, 0.        , 0.04931985,\n",
              "         0.        , 0.        , 0.01115392, 0.04931238, 0.        ,\n",
              "         0.        , 0.04750063],\n",
              "        [0.05020577, 0.        , 0.04047318, 0.01633164, 0.02994521,\n",
              "         0.01250608, 0.04935616, 0.        , 0.        , 0.        ,\n",
              "         0.02683765, 0.0067036 , 0.        , 0.        , 0.0643282 ,\n",
              "         0.        , 0.        , 0.04386348, 0.01552715, 0.        ,\n",
              "         0.        , 0.        , 0.05625538, 0.        , 0.04695395,\n",
              "         0.02429056, 0.07400136, 0.        , 0.01576889, 0.04868954,\n",
              "         0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.01151161, 0.01871461, 0.        ,\n",
              "         0.04103297, 0.02667549, 0.0462786 , 0.        , 0.00561017,\n",
              "         0.03792642, 0.00522944, 0.        , 0.02590381, 0.        ,\n",
              "         0.        , 0.        , 0.03738023, 0.05168883, 0.06301952,\n",
              "         0.        , 0.        , 0.02164565, 0.        , 0.02260429,\n",
              "         0.        , 0.        , 0.02901478, 0.05637617, 0.01614146,\n",
              "         0.        , 0.        ],\n",
              "        [0.0421786 , 0.        , 0.02186011, 0.        , 0.06528088,\n",
              "         0.        , 0.02246265, 0.0191664 , 0.        , 0.00157682,\n",
              "         0.        , 0.0534593 , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05033882, 0.02128597, 0.03751751,\n",
              "         0.        , 0.        , 0.09360813, 0.00997178, 0.        ,\n",
              "         0.        , 0.0818077 , 0.        , 0.05595401, 0.06251238,\n",
              "         0.00044717, 0.0045576 ],\n",
              "        [0.02761544, 0.        , 0.        , 0.01466192, 0.        ,\n",
              "         0.        , 0.        , 0.00174779, 0.        , 0.        ,\n",
              "         0.00652838, 0.0138634 , 0.        , 0.        , 0.        ,\n",
              "         0.02232456, 0.        , 0.00809588, 0.03804559, 0.        ,\n",
              "         0.        , 0.        , 0.05627236, 0.        , 0.02712065,\n",
              "         0.        , 0.        , 0.        , 0.02193855, 0.04646239,\n",
              "         0.01470468, 0.        ],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387],\n",
              "        [0.01040807, 0.        , 0.        , 0.        , 0.03613239,\n",
              "         0.01302801, 0.00876266, 0.00028133, 0.        , 0.        ,\n",
              "         0.        , 0.01680934, 0.        , 0.        , 0.        ,\n",
              "         0.00040104, 0.        , 0.01182513, 0.06257451, 0.        ,\n",
              "         0.00070885, 0.        , 0.0488594 , 0.00245026, 0.        ,\n",
              "         0.        , 0.00862551, 0.        , 0.00761153, 0.01832508,\n",
              "         0.        , 0.01110387]]], dtype=float32)>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_1d_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              "array([[0.10896946, 0.        , 0.04047318, 0.05390075, 0.06528088,\n",
              "        0.04103297, 0.04935616, 0.0462786 , 0.03102152, 0.00561017,\n",
              "        0.03792642, 0.0534593 , 0.        , 0.02590381, 0.0643282 ,\n",
              "        0.02232456, 0.        , 0.05033882, 0.06839722, 0.06301952,\n",
              "        0.00070885, 0.04786102, 0.09360813, 0.00997178, 0.04931985,\n",
              "        0.02429056, 0.0818077 , 0.02901478, 0.05637617, 0.06251238,\n",
              "        0.01470468, 0.04750063]], dtype=float32)>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_pool_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215/215 [==============================] - 5s 20ms/step - loss: 0.1523 - accuracy: 0.9480 - val_loss: 0.7438 - val_accuracy: 0.7822\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1025 - accuracy: 0.9618 - val_loss: 0.8880 - val_accuracy: 0.7769\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.0807 - accuracy: 0.9683 - val_loss: 0.9547 - val_accuracy: 0.7743\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0693 - accuracy: 0.9734 - val_loss: 1.0408 - val_accuracy: 0.7703\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0602 - accuracy: 0.9752 - val_loss: 1.1129 - val_accuracy: 0.7717\n"
          ]
        }
      ],
      "source": [
        "# Create 1D CNN layer to model sequences\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = tf.keras.layers.Conv1D(filters=64,\n",
        "                           kernel_size=5,\n",
        "                           padding=\"valid\",\n",
        "                           activation=\"relu\")(x)\n",
        "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_5 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_5_history = model_5.fit(train_sentences,train_labels,\n",
        "                              validation_data=(val_sentences,val_labels),\n",
        "                              epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2.4610125e-01],\n",
              "       [5.9993929e-01],\n",
              "       [9.9996984e-01],\n",
              "       [4.9765490e-02],\n",
              "       [2.8639894e-07],\n",
              "       [9.9715459e-01],\n",
              "       [9.8548234e-01],\n",
              "       [9.9981397e-01],\n",
              "       [9.9999881e-01],\n",
              "       [7.5610954e-01]], dtype=float32)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_pred_probs = model_5.predict(val_sentences)\n",
        "model_5_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
        "model_5_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 77.16535433070865,\n",
              " 'precision': 0.7722289521502119,\n",
              " 'recall': 0.7716535433070866,\n",
              " 'f1': 0.7701831305177762}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_results = calculate_results(val_labels,model_5_preds)\n",
        "model_5_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 6 - Tensorflow Hub pretrained models\n",
        "USE-extractor - universal sentence encoder (USE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-0.01157027  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n",
            "  0.02680985  0.05589838 -0.01068729 -0.00597292  0.00639323 -0.0181952\n",
            "  0.00030814  0.09105888  0.05874645 -0.03180628  0.01512474 -0.05162929\n",
            "  0.00991367 -0.06865346 -0.04209305  0.0267898   0.03011008  0.00321069\n",
            " -0.00337971 -0.04787356  0.02266719 -0.00985925 -0.04063613 -0.01292093\n",
            " -0.04666384  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014441\n",
            "  0.02871508  0.04947684 -0.00633978 -0.08960193  0.02807117 -0.00808362\n",
            " -0.01360601  0.0599865  -0.10361787 -0.05195374  0.00232955 -0.0233253\n",
            " -0.03758105  0.03327729], shape=(50,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "embed_sample = embed([sample_sentence,\n",
        "                      \"When you can the unvidersal sentence encider on a sentence, it turns into into numbers\"])\n",
        "print(embed_sample[0][:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
              "array([[-0.01157027,  0.0248591 ,  0.02878048, ..., -0.00186125,\n",
              "         0.02315824, -0.01485021],\n",
              "       [ 0.06090042, -0.08857404,  0.01838204, ..., -0.00653699,\n",
              "         0.03106262,  0.03701009]], dtype=float32)>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the above USE\"\" embedding layer for a normal type of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a keras layer using the USE\"\" pretrained layer from tensorflow hub\n",
        "sentence_encoder_layer = hub.KerasLayer(hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\"),\n",
        "                                        input_shape=[],\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow_hub.keras_layer.KerasLayer at 0x7f7a20ae4fd0>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_encoder_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 3s 7ms/step - loss: 0.6482 - accuracy: 0.7378 - val_loss: 0.6154 - val_accuracy: 0.7769\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.5809 - accuracy: 0.7913 - val_loss: 0.5653 - val_accuracy: 0.7795\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.5378 - accuracy: 0.7930 - val_loss: 0.5333 - val_accuracy: 0.7822\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.5091 - accuracy: 0.7973 - val_loss: 0.5119 - val_accuracy: 0.7848\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.4891 - accuracy: 0.7999 - val_loss: 0.4973 - val_accuracy: 0.7887\n"
          ]
        }
      ],
      "source": [
        "# Create  sequential layer\n",
        "model_6 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_6_history = model_6.fit(train_sentences,train_labels,\n",
        "                               validation_data=(val_sentences,val_labels),\n",
        "                               epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.37072343],\n",
              "       [0.67399824],\n",
              "       [0.849519  ],\n",
              "       [0.3633444 ],\n",
              "       [0.6382592 ],\n",
              "       [0.7322892 ],\n",
              "       [0.8246749 ],\n",
              "       [0.84339666],\n",
              "       [0.7467693 ],\n",
              "       [0.1936599 ]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_pred_probs=model_6.predict(val_sentences)\n",
        "model_6_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_preds=tf.squeeze(tf.round(model_6_pred_probs))\n",
        "model_6_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.87139107611549,\n",
              " 'precision': 0.7896925793067333,\n",
              " 'recall': 0.7887139107611548,\n",
              " 'f1': 0.7873013461184764}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_results = calculate_results(val_labels,model_6_preds)\n",
        "model_6_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.4821 - accuracy: 0.7857 - val_loss: 0.4378 - val_accuracy: 0.8005\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.4011 - accuracy: 0.8222 - val_loss: 0.4234 - val_accuracy: 0.8163\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8345 - val_loss: 0.4219 - val_accuracy: 0.8163\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3581 - accuracy: 0.8483 - val_loss: 0.4210 - val_accuracy: 0.8202\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8599 - val_loss: 0.4230 - val_accuracy: 0.8215\n",
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.16142817]\n",
            " [0.79830945]\n",
            " [0.99721247]\n",
            " [0.16722518]\n",
            " [0.6336406 ]\n",
            " [0.86117154]\n",
            " [0.9954034 ]\n",
            " [0.9934882 ]\n",
            " [0.9897069 ]\n",
            " [0.06630458]]\n",
            "tf.Tensor([0. 1. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 82.1522309711286, 'precision': 0.822740944702772, 'recall': 0.821522309711286, 'f1': 0.8204589074026987}\n"
          ]
        }
      ],
      "source": [
        "# Challenge, beat the baseline\n",
        "model_6_exp = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_6_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_6_exp_history = model_6_exp.fit(train_sentences,train_labels,\n",
        "                                      epochs=5,\n",
        "                                      validation_data=(val_sentences,val_labels))\n",
        "\n",
        "model_6_exp_pred_probs = model_6_exp.predict(val_sentences)\n",
        "print(model_6_exp_pred_probs[:10])\n",
        "\n",
        "model_6_exp_preds = tf.squeeze(tf.round(model_6_exp_pred_probs))\n",
        "print(model_6_exp_preds[:10])\n",
        "\n",
        "model_6_exp_results = calculate_results(val_labels, model_6_exp_preds)\n",
        "print(model_6_exp_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 7 : TF Hub Pretrained USE but with 10% of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(761, 761)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
        "train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
        "train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
        "len(train_sentences_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    413\n",
              "1    348\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the number of targets in the dataset\n",
        "train_10_percent[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df_shuffled[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 1s 22ms/step - loss: 0.6699 - accuracy: 0.6413 - val_loss: 0.6290 - val_accuracy: 0.7703\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.5775 - accuracy: 0.8029 - val_loss: 0.5062 - val_accuracy: 0.8215\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.4617 - accuracy: 0.8213 - val_loss: 0.4047 - val_accuracy: 0.8412\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.3875 - accuracy: 0.8423 - val_loss: 0.3459 - val_accuracy: 0.8622\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.3380 - accuracy: 0.8620 - val_loss: 0.3001 - val_accuracy: 0.8780\n"
          ]
        }
      ],
      "source": [
        "# model 7\n",
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_7_history = model_7.fit(train_sentences_10_percent, train_labels_10_percent,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.09183506],\n",
              "       [0.8766855 ],\n",
              "       [0.955973  ],\n",
              "       [0.22970389],\n",
              "       [0.9057463 ],\n",
              "       [0.8609012 ],\n",
              "       [0.95389783],\n",
              "       [0.97209686],\n",
              "       [0.9284693 ],\n",
              "       [0.01393676]], dtype=float32)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "model_7_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "model_7_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 87.79527559055119,\n",
              " 'precision': 0.8787082788740477,\n",
              " 'recall': 0.8779527559055118,\n",
              " 'f1': 0.8775165403027484}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_results = calculate_results(val_labels,model_7_preds)\n",
        "model_7_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model 7 did very good compared to model 6, model is same, but only 10% data, why?\n",
        "what could the reason be:\n",
        "1. maybe the way we created 10% data\n",
        "\n",
        "ANS : train_sentences_10_percent has little amount of val_sentences, becuase we took 10% from whole train_df_shuffled variable\n",
        "\n",
        "So, it is already seeing what it will be validating on.\n",
        "\n",
        "This is called data leak problem, some part of validation/testing data leaks into training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(685, 685)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Making correct data allocation - without data leakage\n",
        "train_10_percent_split = int(0.1 * len(train_sentences))    # train_sentences are already in random order, no need to shuffle\n",
        "train_sentence_10_percent = train_sentences[:train_10_percent_split]\n",
        "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
        "len(train_sentence_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 108ms/step - loss: 0.6687 - accuracy: 0.6219 - val_loss: 0.6524 - val_accuracy: 0.5997\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.5799 - accuracy: 0.7635 - val_loss: 0.5579 - val_accuracy: 0.7717\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4548 - accuracy: 0.8204 - val_loss: 0.4954 - val_accuracy: 0.7940\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3764 - accuracy: 0.8438 - val_loss: 0.5009 - val_accuracy: 0.7808\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3252 - accuracy: 0.8642 - val_loss: 0.5104 - val_accuracy: 0.7730\n"
          ]
        }
      ],
      "source": [
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "mdoel_7_history_split = model_7.fit(train_sentence_10_percent, train_labels_10_percent,\n",
        "                                    epochs=5,\n",
        "                                    validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.02782756]\n",
            " [0.47674778]\n",
            " [0.9636344 ]\n",
            " [0.4345637 ]\n",
            " [0.64019966]\n",
            " [0.678855  ]\n",
            " [0.9527489 ]\n",
            " [0.8586304 ]\n",
            " [0.9380611 ]\n",
            " [0.09271669]]\n",
            "tf.Tensor([0. 0. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 77.29658792650919, 'precision': 0.7738445106757977, 'recall': 0.7729658792650919, 'f1': 0.7713337273803944}\n"
          ]
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "print(model_7_pred_probs[:10])\n",
        "\n",
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "print(model_7_preds[:10])\n",
        "\n",
        "model_7_results = calculate_results(val_labels, model_7_preds)\n",
        "print(model_7_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the performance of each of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_model_results = pd.DataFrame({\"baseline\":baseline_results,\n",
        "                                  \"simple_dense\":model_1_results,\n",
        "                                  \"lstm\":model_2_results})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
