{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKHJ_5qXsNXW"
      },
      "source": [
        "# Introduction to NLP Fundamentals in Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rj7sBkxRK6G",
        "outputId": "1e663cc2-0b3a-43f0-d474-9f7e1e112a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.14.0-dev20230730\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67y0phrRQnz",
        "outputId": "81d04c0b-1f69-419f-f579-b8cd75387b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-13 14:24:35--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-08-13 14:24:36 (12.0 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "  !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "  print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n",
        "\n",
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5L-H4otpgvS"
      },
      "source": [
        "We will be using RNN to train textual information\n",
        "\n",
        "**Basic Structure of RNN**\n",
        "* Input layer\n",
        "* text_vectorizer(input)\n",
        "* embedding(x)\n",
        "* LSTM -> activation = tanh\n",
        "* output_Dense -> activation = sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0eqNb9Nsyzi"
      },
      "source": [
        "### Get the text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Fp9BY2pkCZ",
        "outputId": "68fbdfce-f7fa-4d30-89d2-7ee2bfeeac45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-13 14:24:37--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4007:829::2010, 2404:6800:4007:826::2010, 2404:6800:4007:827::2010, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4007:829::2010|:443... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "nlp_getting_started 100%[===================>] 593.11K   189KB/s    in 3.1s    \n",
            "\n",
            "2023-08-13 14:24:41 (189 KB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
        "    !wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "    unzip_data(\"nlp_getting_started.zip\")\n",
        "else:\n",
        "  print(\"[INFO] already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzD1DRKtRlJ"
      },
      "source": [
        "### Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qyWmNLXTtC_x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BoTo82yJt4no",
        "outputId": "913f97ff-16a7-4f96-c138-4b2126474a9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5ImBTHVft6K2",
        "outputId": "0b19d104-d524-4206-9c13-486aa55e5c34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>7934</td>\n",
              "      <td>rainstorm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@Calum5SOS you look like you got caught in a r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>2538</td>\n",
              "      <td>collision</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my favorite lady came to our volunteer meeting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>2611</td>\n",
              "      <td>crashed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@brianroemmele UX fail of EMV - people want to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6810</th>\n",
              "      <td>9756</td>\n",
              "      <td>tragedy</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Can't find my ariana grande shirt  this is a f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>6254</td>\n",
              "      <td>hijacking</td>\n",
              "      <td>Athens,Greece</td>\n",
              "      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                    NaN   \n",
              "2227  3185       deluge                    NaN   \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                    NaN   \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "5559  7934    rainstorm                    NaN   \n",
              "1765  2538    collision                    NaN   \n",
              "1817  2611      crashed                    NaN   \n",
              "6810  9756      tragedy        Los Angeles, CA   \n",
              "4398  6254    hijacking          Athens,Greece   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "5559  @Calum5SOS you look like you got caught in a r...       0  \n",
              "1765  my favorite lady came to our volunteer meeting...       1  \n",
              "1817  @brianroemmele UX fail of EMV - people want to...       1  \n",
              "6810  Can't find my ariana grande shirt  this is a f...       0  \n",
              "4398  The Murderous Story Of AmericaÛªs First Hijac...       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Shuffle to train data\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "train_df_shuffled.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2wbB2XBBuUsd",
        "outputId": "5d7aa573-919e-4194-a637-848374c73253"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're shaking...It's an earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They'd probably still show more life than Arse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey! How are you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a nice hat?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fuck off!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
              "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
              "7  22     NaN      NaN                                  Hey! How are you?\n",
              "8  27     NaN      NaN                                   What a nice hat?\n",
              "9  29     NaN      NaN                                          Fuck off!"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_data\n",
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNWm50cubJ8",
        "outputId": "660bd2ae-b113-448f-d7fd-7a4a79e2cd2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#How many examples in each class\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA79OdN4umTy",
        "outputId": "d1f9094c-38b4-494b-e75f-d7ee32698c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3870\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "Learning from the Legacy of a Catastrophic Eruption - The New Yorker http://t.co/t344PhNpy9\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "We want to see no more Hiroshima and Nagasaki nuclear bomb disaster in this beautiful world.Lets be peaceful &amp; save this human civilization.\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "On holiday to relax sunbathe and drink ... Putting out bush fires? Not so much ?? #spain https://t.co/dRno7OKM21\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "If you can't have the roar of the waves a rainstorm &amp; some rollingÛ_ https://t.co/DlVYFvnQee\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "Ain't no bags in the trunk it's a body\n",
            "\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# visualise some random training examples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "print(random_index)\n",
        "for row in train_df_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  index,text,target = row\n",
        "  print(f\"Target: {target}, {'(real disaster)' if target > 0 else '(not a real disaster)'}\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRuczOy1Hze",
        "outputId": "7fd6c85e-4713-4d69-e837-61fd589df245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_df_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ilKeV5jwa0E"
      },
      "outputs": [],
      "source": [
        "#Splitting the data to create validation -> (train + validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2LtkKK08Rd",
        "outputId": "7109197e-f115-4f06-8be1-d9913d133cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check length\n",
        "len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZ7B-jT1EHj",
        "outputId": "66494cf4-27da-4c56-e9ee-5a9272a8b1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object),\n",
              " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check few sample\n",
        "train_sentences[:10],train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfc38t53GjO"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "1. tokenization -> can get too big as the number of text sample increases\n",
        "2. Embedding -> richer representation of relationship be between tokens, deeper the realtion between words, more the number/embedding for that token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yru6-5TJ1U17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                                    split=\"whitespace\", # how to split tokens\n",
        "                                                    ngrams=None, # create groups of n-words?\n",
        "                                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_length,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "@Guy_Reginald lol more than welcome ??????      \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   1,  174,   51,   76, 1569,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
          ]
        }
      ],
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.layers.core.embedding.Embedding at 0x7f9cc4f01340>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\") \n",
        "\n",
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "@kaputt21 Hamburg Police Chief Gregory Wickett has told 7 Eyewitness News he 'can't confirm or deny' an investigation is underway.\n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.02615548,  0.03596283, -0.03193288, ..., -0.04215677,\n",
              "         -0.04517215,  0.02450839],\n",
              "        [ 0.04794712, -0.00440628,  0.00558112, ...,  0.02908808,\n",
              "         -0.01055112,  0.03687351],\n",
              "        [-0.00431956, -0.01643995, -0.00313234, ...,  0.02638216,\n",
              "         -0.01434306, -0.04094162],\n",
              "        ...,\n",
              "        [ 0.00403015,  0.03711834, -0.01238756, ..., -0.03173318,\n",
              "          0.02159021,  0.03526375],\n",
              "        [-0.00512243,  0.00519339, -0.02927925, ...,  0.02025126,\n",
              "         -0.01685586, -0.00014712],\n",
              "        [ 0.01464624,  0.01270623, -0.04064868, ...,  0.02356646,\n",
              "          0.03027831,  0.00211463]]], dtype=float32)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([ 0.02615548,  0.03596283, -0.03193288,  0.02808413, -0.01696178,\n",
              "        0.00710521, -0.03592349, -0.02334394, -0.00444476, -0.02355561,\n",
              "        0.03088863, -0.00189409,  0.02129949, -0.00063553,  0.04628089,\n",
              "        0.04599741, -0.01787822,  0.00503946, -0.01076424, -0.04629968,\n",
              "       -0.00728178, -0.03432925, -0.03750493,  0.01832289, -0.02760329,\n",
              "        0.03398905,  0.04847938,  0.00794345, -0.01798973,  0.04390371,\n",
              "        0.00180838, -0.0391475 , -0.02652582,  0.01523193,  0.02101827,\n",
              "       -0.01774635, -0.02174931,  0.00253918,  0.04823767, -0.02367487,\n",
              "        0.04867179,  0.01412921,  0.00623195,  0.03430438,  0.03869802,\n",
              "        0.01941646,  0.03095542,  0.02672033,  0.01740826,  0.01342591,\n",
              "       -0.01146766, -0.02638488,  0.0297741 , -0.03014934, -0.03304125,\n",
              "       -0.00615761, -0.00123803, -0.03858942,  0.02102337,  0.00327063,\n",
              "        0.01224952,  0.02950187, -0.03490544, -0.00267539,  0.01743373,\n",
              "        0.03739259,  0.0251247 ,  0.01418575, -0.0462628 ,  0.01509268,\n",
              "        0.01025537,  0.0094296 ,  0.0208515 ,  0.02503934, -0.0051287 ,\n",
              "       -0.03146098,  0.00264881,  0.00472422,  0.03362303, -0.00953249,\n",
              "       -0.03136503,  0.01978364,  0.04204402,  0.01330299, -0.04239191,\n",
              "       -0.02256707,  0.00029955, -0.04203439,  0.02301873, -0.00157116,\n",
              "        0.01792816, -0.02378924,  0.033956  ,  0.02743442, -0.01788329,\n",
              "        0.04690112, -0.02717031,  0.01230022, -0.02029388,  0.04141123,\n",
              "        0.01851371,  0.04402887, -0.02878845,  0.04822068,  0.02096592,\n",
              "       -0.00377386,  0.03090615,  0.04661231, -0.0258198 , -0.04526217,\n",
              "        0.02437354,  0.02870684,  0.00791981, -0.04708444, -0.03535801,\n",
              "        0.02723617,  0.0469034 , -0.03001962, -0.02124964, -0.00348619,\n",
              "       -0.00489842,  0.03605865, -0.04651858, -0.04589653, -0.01674353,\n",
              "       -0.04215677, -0.04517215,  0.02450839], dtype=float32)>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelling a text dataset\n",
        "More specifically, we'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model 0\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our baseline model achieves an accuracy of: 79.27%\n"
          ]
        }
      ],
      "source": [
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_functions import calculate_results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 1: A simple dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1280129 (4.88 MB)\n",
            "Trainable params: 1280129 (4.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create directory to save TensorBoard logs\n",
        "SAVE_DIR = \"model_logs\"\n",
        "\n",
        "# Build model with the Functional API\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into numbers\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
        "# Compile model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/simple_dense_model/20230813-142442\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 17ms/step - loss: 0.6092 - accuracy: 0.6920 - val_loss: 0.5359 - val_accuracy: 0.7559\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.4408 - accuracy: 0.8199 - val_loss: 0.4693 - val_accuracy: 0.7887\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.3461 - accuracy: 0.8622 - val_loss: 0.4592 - val_accuracy: 0.7913\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2846 - accuracy: 0.8918 - val_loss: 0.4644 - val_accuracy: 0.7874\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2377 - accuracy: 0.9121 - val_loss: 0.4770 - val_accuracy: 0.7861\n"
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
        "                                                                     experiment_name=\"simple_dense_model\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.7861\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4769628345966339, 0.7860892415046692]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recurrent Neural Network\n",
        "* RNN's are useful for sequence data\n",
        "* The premise of a recurrent neural network is to use the representation of previous input to aid the representation of a later input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 : LSTM -> Long short term memory\n",
        "\n",
        "Input(text) -> Toeknize -> Embedding -> Layers(RNN/Dense) -> Output(label probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 15, 128)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 64)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 15, 64)            49408     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 15, 64)            33024     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1395521 (5.32 MB)\n",
            "Trainable params: 1395521 (5.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Set random seed and create embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_2_embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.LSTM(64)(x) # return vector for whole sequence\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20230813-142450\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215/215 [==============================] - 7s 21ms/step - loss: 0.5099 - accuracy: 0.7466 - val_loss: 0.4628 - val_accuracy: 0.7861\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3252 - accuracy: 0.8701 - val_loss: 0.5121 - val_accuracy: 0.7782\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.2284 - accuracy: 0.9145 - val_loss: 0.5555 - val_accuracy: 0.7598\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1641 - accuracy: 0.9409 - val_loss: 0.6742 - val_accuracy: 0.7835\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1150 - accuracy: 0.9539 - val_loss: 0.8305 - val_accuracy: 0.7625\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "model_2_history = model_2.fit(train_sentences, train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
        "                                                                     \"LSTM\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.03900822],\n",
              "       [0.6284077 ],\n",
              "       [0.9994671 ],\n",
              "       [0.13896386],\n",
              "       [0.00195334],\n",
              "       [0.9995714 ],\n",
              "       [0.965141  ],\n",
              "       [0.99964416],\n",
              "       [0.9995028 ],\n",
              "       [0.17703865]], dtype=float32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert preds to laels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.24671916010499,\n",
              " 'precision': 0.7631352147335341,\n",
              " 'recall': 0.7624671916010499,\n",
              " 'f1': 0.7607595644846901}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_results = calculate_results(val_labels, model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 3: GRU\n",
        "\n",
        "1. Gated recurrent unit is effective and popular\n",
        "2. GRU cell has similar features to LSTM cell but has less paramenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215/215 [==============================] - 5s 18ms/step - loss: 0.2360 - accuracy: 0.9012 - val_loss: 0.5336 - val_accuracy: 0.7756\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1602 - accuracy: 0.9421 - val_loss: 0.5937 - val_accuracy: 0.7782\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1285 - accuracy: 0.9531 - val_loss: 0.7514 - val_accuracy: 0.7874\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1099 - accuracy: 0.9599 - val_loss: 0.7547 - val_accuracy: 0.7874\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0924 - accuracy: 0.9664 - val_loss: 0.7260 - val_accuracy: 0.7887\n"
          ]
        }
      ],
      "source": [
        "# Build GRU RNN\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "# x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.GRU(64)(x)\n",
        "# x = tf.keras.layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                        train_labels,\n",
        "                        epochs=5,\n",
        "                        validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1/24 [>.............................] - ETA: 3s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.0600846 ],\n",
              "       [0.8264787 ],\n",
              "       [0.9996037 ],\n",
              "       [0.04250248],\n",
              "       [0.00101213],\n",
              "       [0.98864347],\n",
              "       [0.70060784],\n",
              "       [0.9997768 ],\n",
              "       [0.99950564],\n",
              "       [0.6669447 ]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert model 3 pred_probs to labels\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.87139107611549,\n",
              " 'precision': 0.7909484312997928,\n",
              " 'recall': 0.7887139107611548,\n",
              " 'f1': 0.7867378765635805}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_results = calculate_results(val_labels,model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model4 - Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 22ms/step - loss: 0.5066 - accuracy: 0.7510 - val_loss: 0.4539 - val_accuracy: 0.7848\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.3115 - accuracy: 0.8770 - val_loss: 0.5177 - val_accuracy: 0.7664\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.2068 - accuracy: 0.9223 - val_loss: 0.5564 - val_accuracy: 0.7651\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1411 - accuracy: 0.9545 - val_loss: 0.6678 - val_accuracy: 0.7612\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.0993 - accuracy: 0.9664 - val_loss: 0.7840 - val_accuracy: 0.7507\n"
          ]
        }
      ],
      "source": [
        "# Trying out on my own\n",
        "text_vectorizer_exp = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                      output_sequence_length=15)\n",
        "text_vectorizer_exp.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer_exp(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4_exp = tf.keras.Model(inputs,outputs)\n",
        "model_4_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_4_history_exp = model_4_exp.fit(train_sentences,\n",
        "                                      train_labels,\n",
        "                                      validation_data=(val_sentences,val_labels),\n",
        "                                      epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normal RNN's go from left to right\n",
        "\n",
        "Bi-directional go from right to left as well as left to right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 6s 19ms/step - loss: 0.5063 - accuracy: 0.7497 - val_loss: 0.4580 - val_accuracy: 0.7822\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8745 - val_loss: 0.5133 - val_accuracy: 0.7717\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.2145 - accuracy: 0.9186 - val_loss: 0.5587 - val_accuracy: 0.7703\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1527 - accuracy: 0.9479 - val_loss: 0.6196 - val_accuracy: 0.7769\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9634 - val_loss: 0.6794 - val_accuracy: 0.7835\n"
          ]
        }
      ],
      "source": [
        "# model 4\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                                    output_sequence_length=15)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_4_history = model_4.fit(train_sentences,train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.03953835],\n",
              "       [0.8144705 ],\n",
              "       [0.99836904],\n",
              "       [0.16232441],\n",
              "       [0.00539128],\n",
              "       [0.98951423],\n",
              "       [0.8664646 ],\n",
              "       [0.9993222 ],\n",
              "       [0.9992299 ],\n",
              "       [0.2928263 ]], dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.34645669291339,\n",
              " 'precision': 0.7831762038971263,\n",
              " 'recall': 0.7834645669291339,\n",
              " 'f1': 0.7832103815590454}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_results = calculate_results(val_labels,model_4_preds)\n",
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
        "conv_1d = tf.keras.layers.Conv1D(32,\n",
        "                                 5,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding=\"valid\")\n",
        "conv_1d_output = conv_1d(embedding_test)\n",
        "\n",
        "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output)\n",
        "\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.05263913, -0.06186633, -0.02827875, ..., -0.0233285 ,\n",
              "         -0.01306801, -0.00136552],\n",
              "        [-0.06288031, -0.0706658 , -0.01494047, ...,  0.03592439,\n",
              "          0.03072703, -0.02100245],\n",
              "        [-0.0558992 ,  0.01345649, -0.03325709, ...,  0.03489795,\n",
              "         -0.02885491, -0.03427256],\n",
              "        ...,\n",
              "        [-0.00209794, -0.00416578, -0.05000888, ...,  0.02023026,\n",
              "         -0.0032082 ,  0.04568345],\n",
              "        [-0.00209794, -0.00416578, -0.05000888, ...,  0.02023026,\n",
              "         -0.0032082 ,  0.04568345],\n",
              "        [-0.00209794, -0.00416578, -0.05000888, ...,  0.02023026,\n",
              "         -0.0032082 ,  0.04568345]]], dtype=float32)>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
              "array([[[1.80814900e-02, 3.63051780e-02, 0.00000000e+00, 5.31983078e-02,\n",
              "         3.78285088e-02, 0.00000000e+00, 0.00000000e+00, 7.32593238e-02,\n",
              "         0.00000000e+00, 0.00000000e+00, 1.66051202e-02, 5.78369796e-02,\n",
              "         0.00000000e+00, 9.51729491e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.40495056e-02,\n",
              "         0.00000000e+00, 7.57319704e-02, 7.61710294e-03, 3.38865966e-02,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.17426009e-03],\n",
              "        [0.00000000e+00, 3.95292677e-02, 0.00000000e+00, 3.58461961e-02,\n",
              "         0.00000000e+00, 0.00000000e+00, 2.01272499e-02, 3.19474936e-03,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.65492511e-02,\n",
              "         1.08705781e-01, 0.00000000e+00, 0.00000000e+00, 2.11508628e-02,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.96843895e-02,\n",
              "         0.00000000e+00, 0.00000000e+00, 5.04624061e-02, 2.94275265e-02,\n",
              "         0.00000000e+00, 6.35472983e-02, 0.00000000e+00, 9.53593478e-03],\n",
              "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.93792503e-02,\n",
              "         3.28416638e-02, 1.34298168e-02, 0.00000000e+00, 5.60224950e-02,\n",
              "         2.42514387e-02, 0.00000000e+00, 5.74352294e-02, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.17299739e-02,\n",
              "         0.00000000e+00, 2.26103850e-02, 0.00000000e+00, 3.52581963e-02,\n",
              "         6.33243471e-05, 8.00751336e-03, 3.55623960e-02, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.23728658e-02, 0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 9.53561664e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 5.65827265e-02, 0.00000000e+00, 2.68046074e-02,\n",
              "         8.88132863e-03, 3.91381644e-02, 2.20285524e-02, 4.62330133e-03,\n",
              "         0.00000000e+00, 0.00000000e+00, 7.50218704e-03, 1.06588528e-02,\n",
              "         1.80943236e-02, 0.00000000e+00, 0.00000000e+00, 2.35445239e-02,\n",
              "         0.00000000e+00, 7.53766447e-02, 0.00000000e+00, 9.46563296e-03,\n",
              "         0.00000000e+00, 0.00000000e+00, 2.62439325e-02, 0.00000000e+00,\n",
              "         0.00000000e+00, 1.18612051e-02, 0.00000000e+00, 2.69450061e-02],\n",
              "        [0.00000000e+00, 1.01582119e-02, 8.77979957e-03, 4.22344655e-02,\n",
              "         0.00000000e+00, 6.93579577e-03, 0.00000000e+00, 3.57064940e-02,\n",
              "         2.30380893e-03, 0.00000000e+00, 0.00000000e+00, 1.75487977e-02,\n",
              "         0.00000000e+00, 4.32931371e-02, 0.00000000e+00, 8.59574694e-03,\n",
              "         0.00000000e+00, 2.51691602e-02, 0.00000000e+00, 2.24891268e-02,\n",
              "         0.00000000e+00, 3.55863757e-02, 0.00000000e+00, 4.40365560e-02,\n",
              "         0.00000000e+00, 1.94045417e-02, 2.87945867e-02, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.47070163e-03],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02],\n",
              "        [0.00000000e+00, 2.64225267e-02, 0.00000000e+00, 2.50612833e-02,\n",
              "         3.60359438e-03, 4.68287431e-03, 0.00000000e+00, 3.77012901e-02,\n",
              "         1.14471475e-02, 0.00000000e+00, 0.00000000e+00, 4.26567718e-02,\n",
              "         0.00000000e+00, 6.59346534e-03, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 3.22772861e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 4.24167290e-02, 2.83833966e-03, 0.00000000e+00,\n",
              "         2.95543112e-03, 7.88434409e-03, 3.32766026e-02, 8.79643299e-03,\n",
              "         0.00000000e+00, 2.18688324e-02, 0.00000000e+00, 2.00798586e-02]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_1d_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              "array([[0.01808149, 0.09535617, 0.0087798 , 0.05319831, 0.03782851,\n",
              "        0.05658273, 0.02012725, 0.07325932, 0.02425144, 0.03913816,\n",
              "        0.05743523, 0.05783698, 0.10870578, 0.09517295, 0.00750219,\n",
              "        0.02115086, 0.01809432, 0.03227729, 0.        , 0.03172997,\n",
              "        0.        , 0.07537664, 0.00283834, 0.08968439, 0.00295543,\n",
              "        0.07573197, 0.05046241, 0.0338866 , 0.        , 0.0635473 ,\n",
              "        0.        , 0.02694501]], dtype=float32)>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_pool_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trying on my own\n",
        "# text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "#                                                     output_sequence_length=15)\n",
        "# text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "# inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "# x = text_vectorizer(inputs)\n",
        "# x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "#                               output_dim=128,\n",
        "#                               input_length=15)(x)\n",
        "# x = tf.keras.layers.Conv1D(64,3)(x)\n",
        "# x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
        "# x = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "# model_5_exp = tf.keras.Model(inputs,outputs)\n",
        "# model_5_exp.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
