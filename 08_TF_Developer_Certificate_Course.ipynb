{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rj7sBkxRK6G",
        "outputId": "1e663cc2-0b3a-43f0-d474-9f7e1e112a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "2.14.0-dev20230730\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67y0phrRQnz",
        "outputId": "81d04c0b-1f69-419f-f579-b8cd75387b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 'helper_functions.py' already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "  !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "  print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n",
        "\n",
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5L-H4otpgvS"
      },
      "source": [
        "We will be using RNN to train textual information\n",
        "\n",
        "**Basic Structure of RNN**\n",
        "* Input layer\n",
        "* text_vectorizer(input)\n",
        "* embedding(x)\n",
        "* LSTM -> activation = tanh\n",
        "* output_Dense -> activation = sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0eqNb9Nsyzi"
      },
      "source": [
        "### Get the text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Fp9BY2pkCZ",
        "outputId": "68fbdfce-f7fa-4d30-89d2-7ee2bfeeac45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
        "    !wget https://storage.googlqeapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "    unzip_data(\"nlp_getting_started.zip\")\n",
        "else:\n",
        "  print(\"[INFO] already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzD1DRKtRlJ"
      },
      "source": [
        "### Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qyWmNLXTtC_x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BoTo82yJt4no",
        "outputId": "913f97ff-16a7-4f96-c138-4b2126474a9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5ImBTHVft6K2",
        "outputId": "0b19d104-d524-4206-9c13-486aa55e5c34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>7934</td>\n",
              "      <td>rainstorm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@Calum5SOS you look like you got caught in a r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>2538</td>\n",
              "      <td>collision</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my favorite lady came to our volunteer meeting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>2611</td>\n",
              "      <td>crashed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@brianroemmele UX fail of EMV - people want to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6810</th>\n",
              "      <td>9756</td>\n",
              "      <td>tragedy</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Can't find my ariana grande shirt  this is a f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>6254</td>\n",
              "      <td>hijacking</td>\n",
              "      <td>Athens,Greece</td>\n",
              "      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                    NaN   \n",
              "2227  3185       deluge                    NaN   \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                    NaN   \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "5559  7934    rainstorm                    NaN   \n",
              "1765  2538    collision                    NaN   \n",
              "1817  2611      crashed                    NaN   \n",
              "6810  9756      tragedy        Los Angeles, CA   \n",
              "4398  6254    hijacking          Athens,Greece   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "5559  @Calum5SOS you look like you got caught in a r...       0  \n",
              "1765  my favorite lady came to our volunteer meeting...       1  \n",
              "1817  @brianroemmele UX fail of EMV - people want to...       1  \n",
              "6810  Can't find my ariana grande shirt  this is a f...       0  \n",
              "4398  The Murderous Story Of AmericaÛªs First Hijac...       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Shuffle to train data\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "train_df_shuffled.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2wbB2XBBuUsd",
        "outputId": "5d7aa573-919e-4194-a637-848374c73253"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're shaking...It's an earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They'd probably still show more life than Arse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey! How are you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a nice hat?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fuck off!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
              "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
              "7  22     NaN      NaN                                  Hey! How are you?\n",
              "8  27     NaN      NaN                                   What a nice hat?\n",
              "9  29     NaN      NaN                                          Fuck off!"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_data\n",
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNWm50cubJ8",
        "outputId": "660bd2ae-b113-448f-d7fd-7a4a79e2cd2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#How many examples in each class\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA79OdN4umTy",
        "outputId": "d1f9094c-38b4-494b-e75f-d7ee32698c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4664\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "@bentossell @ProductHunt Thanks! I know you all get inundated. Have a good one!\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "Suicide bomber kills 15 in Saudi security site mosque - A suicide bomber killed at least 15 people in an attack on... http://t.co/FY0r9o7Xsl\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "What you gonna do now puppies?! No more destroying my #iPhone Lightning cables! https://t.co/Z4jyHaRreW\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "Rescue of the day: ItÛªs World Cat Day Saturday Aug 8th Black Black/White Cats just $5 http://t.co/oqb7DaSMVy #NewBeginningsAnimalRescue\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "LLF TALK WORLD NEWS U.S. in record hurricane drought - The United States hasn't been hit by a major hurricane in t... http://t.co/oqeq4ueGF8\n",
            "\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# visualise some random training examples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "print(random_index)\n",
        "for row in train_df_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  index,text,target = row\n",
        "  print(f\"Target: {target}, {'(real disaster)' if target > 0 else '(not a real disaster)'}\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRuczOy1Hze",
        "outputId": "7fd6c85e-4713-4d69-e837-61fd589df245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_df_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ilKeV5jwa0E"
      },
      "outputs": [],
      "source": [
        "#Splitting the data to create validation -> (train + validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2LtkKK08Rd",
        "outputId": "7109197e-f115-4f06-8be1-d9913d133cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check length\n",
        "len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZ7B-jT1EHj",
        "outputId": "66494cf4-27da-4c56-e9ee-5a9272a8b1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object),\n",
              " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check few sample\n",
        "train_sentences[:10],train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfc38t53GjO"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "1. tokenization -> can get too big as the number of text sample increases\n",
        "2. Embedding -> richer representation of relationship be between tokens, deeper the realtion between words, more the number/embedding for that token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yru6-5TJ1U17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                                    split=\"whitespace\", # how to split tokens\n",
        "                                                    ngrams=None, # create groups of n-words?\n",
        "                                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_length,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "4 police officers arrested for abusing children at police-run boot camp in San Luis Obispo Calif. - @ABC7: http://t.co/QpWOtugUI9      \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[ 178,   77, 1802, 1043,   10, 6279,  656,   17, 4892, 3143,  906,\n",
              "           4, 1483, 3661, 5010]])>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
          ]
        }
      ],
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.layers.core.embedding.Embedding at 0x7f041a184040>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\") \n",
        "\n",
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Ready for my close up... Errrr nope!! #notgoingoutinthat #hailstorm #alberta @HellOnWheelsAMC @HoW_fans @TalkingHell http://t.co/9gIAXD6JTY\n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.04530041, -0.03718175, -0.04787734, ...,  0.03177131,\n",
              "          0.01306747, -0.0456303 ],\n",
              "        [-0.03980108, -0.00265955,  0.01477677, ..., -0.00315977,\n",
              "          0.04630092, -0.00706939],\n",
              "        [-0.01670755,  0.04070539, -0.03500526, ..., -0.04064985,\n",
              "         -0.0115184 , -0.00689869],\n",
              "        ...,\n",
              "        [ 0.01137346, -0.02861202,  0.02830474, ..., -0.03788811,\n",
              "          0.03953104, -0.02667246],\n",
              "        [-0.00179715, -0.01624858,  0.00894459, ..., -0.0068845 ,\n",
              "          0.01142391, -0.01618128],\n",
              "        [ 0.02988626,  0.02892124, -0.02413566, ..., -0.03052253,\n",
              "          0.04260964, -0.00118957]]], dtype=float32)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([-0.04530041, -0.03718175, -0.04787734, -0.01985284, -0.01174311,\n",
              "       -0.0439656 ,  0.01763396,  0.01101271,  0.03258898, -0.01892912,\n",
              "       -0.00822784,  0.01952906, -0.02057079,  0.00248396, -0.03751434,\n",
              "        0.01955361,  0.03215617, -0.0239815 ,  0.04430691, -0.02124251,\n",
              "       -0.02729571, -0.04228146,  0.03003364,  0.02528096,  0.02786819,\n",
              "        0.03309036,  0.02258262,  0.00889023, -0.02649533,  0.04059071,\n",
              "       -0.01184011, -0.01214081, -0.00948421,  0.03163209, -0.00110202,\n",
              "       -0.00673227, -0.01401932, -0.01439838,  0.00687858, -0.0447518 ,\n",
              "       -0.00522581, -0.00060268, -0.00066794, -0.00179669,  0.0276759 ,\n",
              "       -0.0255358 ,  0.03299541, -0.03814336, -0.03528609,  0.03291195,\n",
              "       -0.04451732,  0.02400451,  0.0496111 ,  0.03007443,  0.02950222,\n",
              "       -0.02703453, -0.0311581 , -0.03090551, -0.00940969, -0.04996152,\n",
              "        0.01821226, -0.03301375, -0.00714928, -0.02808573, -0.00023274,\n",
              "        0.03024763, -0.04422388,  0.00469613, -0.00760425, -0.04023056,\n",
              "        0.04942627,  0.02566955, -0.04479343,  0.01021181, -0.01254647,\n",
              "        0.01726789, -0.01040162,  0.02994747,  0.03887225, -0.01080609,\n",
              "        0.04613206, -0.01753579,  0.04442937, -0.01439767,  0.02785304,\n",
              "        0.04927785,  0.02052257, -0.04590994, -0.03087349,  0.00905032,\n",
              "       -0.04035123,  0.0405564 , -0.03244986,  0.02587554,  0.03931076,\n",
              "       -0.0494614 , -0.00813401,  0.00809687,  0.03778842,  0.01710646,\n",
              "       -0.03300281, -0.00877742,  0.02496171,  0.0140638 , -0.00405586,\n",
              "       -0.00066934,  0.01772244, -0.049088  ,  0.01567448,  0.03113866,\n",
              "        0.01315925, -0.00685931,  0.04282084, -0.04716926, -0.04634812,\n",
              "        0.04073557,  0.00914452,  0.03823409,  0.04238137, -0.0247416 ,\n",
              "        0.03921957, -0.02497478, -0.02198473, -0.02815635, -0.00049902,\n",
              "        0.03177131,  0.01306747, -0.0456303 ], dtype=float32)>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelling a text dataset\n",
        "More specifically, we'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model 0\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our baseline model achieves an accuracy of: 79.27%\n"
          ]
        }
      ],
      "source": [
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_functions import calculate_results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 1: A simple dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1280129 (4.88 MB)\n",
            "Trainable params: 1280129 (4.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create directory to save TensorBoard logs\n",
        "SAVE_DIR = \"model_logs\"\n",
        "\n",
        "# Build model with the Functional API\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into numbers\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
        "# Compile model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/simple_dense_model/20230817-003433\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 17ms/step - loss: 0.6099 - accuracy: 0.6925 - val_loss: 0.5363 - val_accuracy: 0.7533\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.4410 - accuracy: 0.8184 - val_loss: 0.4697 - val_accuracy: 0.7874\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.3460 - accuracy: 0.8618 - val_loss: 0.4598 - val_accuracy: 0.7900\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2845 - accuracy: 0.8929 - val_loss: 0.4651 - val_accuracy: 0.7900\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.2376 - accuracy: 0.9121 - val_loss: 0.4777 - val_accuracy: 0.7861\n"
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
        "                                                                     experiment_name=\"simple_dense_model\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4777 - accuracy: 0.7861\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.47767698764801025, 0.7860892415046692]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recurrent Neural Network\n",
        "* RNN's are useful for sequence data\n",
        "* The premise of a recurrent neural network is to use the representation of previous input to aid the representation of a later input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 : LSTM -> Long short term memory\n",
        "\n",
        "Input(text) -> Toeknize -> Embedding -> Layers(RNN/Dense) -> Output(label probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 15, 128)\n",
            "(None, 64)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (Text  (None, 15)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 15, 64)            49408     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 15, 64)            33024     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1395521 (5.32 MB)\n",
            "Trainable params: 1395521 (5.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Set random seed and create embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_2_embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.LSTM(64)(x) # return vector for whole sequence\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20230817-003442\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 22ms/step - loss: 0.5075 - accuracy: 0.7500 - val_loss: 0.4574 - val_accuracy: 0.7874\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.3269 - accuracy: 0.8686 - val_loss: 0.5041 - val_accuracy: 0.7769\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.2283 - accuracy: 0.9145 - val_loss: 0.5552 - val_accuracy: 0.7638\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1682 - accuracy: 0.9394 - val_loss: 0.6153 - val_accuracy: 0.7730\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1156 - accuracy: 0.9561 - val_loss: 0.9079 - val_accuracy: 0.7612\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "model_2_history = model_2.fit(train_sentences, train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
        "                                                                     \"LSTM\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2.7288026e-03],\n",
              "       [6.5842372e-01],\n",
              "       [9.9943787e-01],\n",
              "       [5.4361731e-02],\n",
              "       [7.3983939e-04],\n",
              "       [9.9955779e-01],\n",
              "       [9.2246741e-01],\n",
              "       [9.9963605e-01],\n",
              "       [9.9958986e-01],\n",
              "       [1.0099122e-01]], dtype=float32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert preds to laels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.11548556430446,\n",
              " 'precision': 0.7639062094795714,\n",
              " 'recall': 0.7611548556430446,\n",
              " 'f1': 0.758270611394164}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_results = calculate_results(val_labels, model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 3: GRU\n",
        "\n",
        "1. Gated recurrent unit is effective and popular\n",
        "2. GRU cell has similar features to LSTM cell but has less paramenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 18ms/step - loss: 0.2328 - accuracy: 0.9073 - val_loss: 0.5303 - val_accuracy: 0.7756\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1606 - accuracy: 0.9416 - val_loss: 0.5950 - val_accuracy: 0.7795\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1291 - accuracy: 0.9534 - val_loss: 0.7357 - val_accuracy: 0.7782\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1102 - accuracy: 0.9599 - val_loss: 0.7519 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0925 - accuracy: 0.9664 - val_loss: 0.7313 - val_accuracy: 0.7887\n"
          ]
        }
      ],
      "source": [
        "# Build GRU RNN\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "# x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.GRU(64)(x)\n",
        "# x = tf.keras.layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                        train_labels,\n",
        "                        epochs=5,\n",
        "                        validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[6.1086845e-02],\n",
              "       [8.2203412e-01],\n",
              "       [9.9946803e-01],\n",
              "       [4.9206026e-02],\n",
              "       [9.7508647e-04],\n",
              "       [9.9211991e-01],\n",
              "       [7.4264663e-01],\n",
              "       [9.9968922e-01],\n",
              "       [9.9940062e-01],\n",
              "       [6.5311617e-01]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert model 3 pred_probs to labels\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.87139107611549,\n",
              " 'precision': 0.7909484312997928,\n",
              " 'recall': 0.7887139107611548,\n",
              " 'f1': 0.7867378765635805}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_results = calculate_results(val_labels,model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model4 - Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 22ms/step - loss: 0.5063 - accuracy: 0.7489 - val_loss: 0.4577 - val_accuracy: 0.7900\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.3112 - accuracy: 0.8734 - val_loss: 0.5214 - val_accuracy: 0.7664\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.2052 - accuracy: 0.9219 - val_loss: 0.5620 - val_accuracy: 0.7572\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1387 - accuracy: 0.9539 - val_loss: 0.6657 - val_accuracy: 0.7743\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1010 - accuracy: 0.9673 - val_loss: 0.7606 - val_accuracy: 0.7585\n"
          ]
        }
      ],
      "source": [
        "# Trying out on my own\n",
        "text_vectorizer_exp = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                      output_sequence_length=15)\n",
        "text_vectorizer_exp.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer_exp(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4_exp = tf.keras.Model(inputs,outputs)\n",
        "model_4_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_4_history_exp = model_4_exp.fit(train_sentences,\n",
        "                                      train_labels,\n",
        "                                      validation_data=(val_sentences,val_labels),\n",
        "                                      epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normal RNN's go from left to right\n",
        "\n",
        "Bi-directional go from right to left as well as left to right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 6s 20ms/step - loss: 0.5097 - accuracy: 0.7453 - val_loss: 0.4582 - val_accuracy: 0.7795\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3122 - accuracy: 0.8742 - val_loss: 0.5194 - val_accuracy: 0.7717\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.2106 - accuracy: 0.9180 - val_loss: 0.5567 - val_accuracy: 0.7717\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.9520 - val_loss: 0.6651 - val_accuracy: 0.7782\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.1005 - accuracy: 0.9648 - val_loss: 0.7450 - val_accuracy: 0.7546\n"
          ]
        }
      ],
      "source": [
        "# model 4\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                                    output_sequence_length=15)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_4_history = model_4.fit(train_sentences,train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.07420877],\n",
              "       [0.9186039 ],\n",
              "       [0.99963546],\n",
              "       [0.3463921 ],\n",
              "       [0.00563478],\n",
              "       [0.99840456],\n",
              "       [0.97668844],\n",
              "       [0.9998517 ],\n",
              "       [0.9998735 ],\n",
              "       [0.3323221 ]], dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 75.45931758530183,\n",
              " 'precision': 0.7549177216974436,\n",
              " 'recall': 0.7545931758530183,\n",
              " 'f1': 0.754722914278016}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_results = calculate_results(val_labels,model_4_preds)\n",
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
        "conv_1d = tf.keras.layers.Conv1D(32,\n",
        "                                 5,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding=\"valid\")\n",
        "conv_1d_output = conv_1d(embedding_test)\n",
        "\n",
        "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output)\n",
        "\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.03054602,  0.00565677,  0.01073353, ...,  0.01410936,\n",
              "          0.01715625, -0.00251443],\n",
              "        [-0.00534654, -0.03359807, -0.04551395, ..., -0.035409  ,\n",
              "         -0.05758198, -0.0357897 ],\n",
              "        [ 0.00521703, -0.04901723,  0.01754445, ..., -0.01807433,\n",
              "         -0.0152657 , -0.01019085],\n",
              "        ...,\n",
              "        [ 0.02775339,  0.00940752, -0.02210233, ..., -0.01369841,\n",
              "          0.02535246,  0.00544855],\n",
              "        [ 0.02775339,  0.00940752, -0.02210233, ..., -0.01369841,\n",
              "          0.02535246,  0.00544855],\n",
              "        [ 0.02775339,  0.00940752, -0.02210233, ..., -0.01369841,\n",
              "          0.02535246,  0.00544855]]], dtype=float32)>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
              "array([[[0.00629101, 0.00173167, 0.        , 0.        , 0.        ,\n",
              "         0.0435127 , 0.03820759, 0.07013372, 0.05012494, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.02892926, 0.00302002, 0.05109669, 0.06482902,\n",
              "         0.        , 0.        , 0.02804936, 0.        , 0.06431925,\n",
              "         0.05776182, 0.05861841, 0.07202369, 0.00463436, 0.        ,\n",
              "         0.        , 0.09544349],\n",
              "        [0.02641745, 0.        , 0.00067625, 0.03006024, 0.00116641,\n",
              "         0.10010636, 0.02346494, 0.05440895, 0.06995148, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.05630912, 0.02899412,\n",
              "         0.00131629, 0.        , 0.        , 0.02363124, 0.        ,\n",
              "         0.02152163, 0.        , 0.03682813, 0.        , 0.        ,\n",
              "         0.03388753, 0.01818076, 0.00953974, 0.        , 0.        ,\n",
              "         0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.02641664,\n",
              "         0.03832417, 0.        , 0.08984242, 0.042889  , 0.        ,\n",
              "         0.        , 0.00830575, 0.        , 0.03546688, 0.01402386,\n",
              "         0.        , 0.03287478, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.04656328, 0.00247151, 0.03180479,\n",
              "         0.        , 0.04452375, 0.10078599, 0.        , 0.        ,\n",
              "         0.        , 0.05956726],\n",
              "        [0.00753104, 0.05082145, 0.        , 0.00234232, 0.        ,\n",
              "         0.        , 0.        , 0.11416233, 0.        , 0.00814037,\n",
              "         0.        , 0.        , 0.03608862, 0.        , 0.        ,\n",
              "         0.04217788, 0.        , 0.01209004, 0.00033559, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.03323977, 0.        ,\n",
              "         0.0044517 , 0.02673626, 0.03339034, 0.00834315, 0.04191744,\n",
              "         0.00212417, 0.        ],\n",
              "        [0.01352637, 0.01810625, 0.01501559, 0.        , 0.        ,\n",
              "         0.01884929, 0.00975664, 0.03001637, 0.00482778, 0.        ,\n",
              "         0.        , 0.01483488, 0.        , 0.02320916, 0.        ,\n",
              "         0.        , 0.        , 0.0507023 , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.01639828, 0.        , 0.05422136,\n",
              "         0.        , 0.03844645, 0.02635963, 0.        , 0.02009152,\n",
              "         0.01149672, 0.03787734],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861],\n",
              "        [0.02066757, 0.03443427, 0.03244704, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.05057365, 0.00260947, 0.00631265,\n",
              "         0.        , 0.00767893, 0.0183742 , 0.00552716, 0.        ,\n",
              "         0.        , 0.        , 0.00482579, 0.        , 0.        ,\n",
              "         0.00091932, 0.        , 0.        , 0.        , 0.02940748,\n",
              "         0.02497683, 0.04646219, 0.08249085, 0.        , 0.0044329 ,\n",
              "         0.00415465, 0.02063861]]], dtype=float32)>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_1d_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              "array([[0.02641745, 0.05082145, 0.03244704, 0.03006024, 0.02641664,\n",
              "        0.10010636, 0.03820759, 0.11416233, 0.06995148, 0.00814037,\n",
              "        0.        , 0.01483488, 0.03608862, 0.05630912, 0.02899412,\n",
              "        0.04217788, 0.03287478, 0.0507023 , 0.05109669, 0.06482902,\n",
              "        0.02152163, 0.        , 0.04656328, 0.03323977, 0.06431925,\n",
              "        0.05776182, 0.05861841, 0.10078599, 0.00834315, 0.04191744,\n",
              "        0.01149672, 0.09544349]], dtype=float32)>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_pool_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 19ms/step - loss: 0.1530 - accuracy: 0.9457 - val_loss: 0.7502 - val_accuracy: 0.7887\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.1020 - accuracy: 0.9626 - val_loss: 0.8878 - val_accuracy: 0.7756\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0802 - accuracy: 0.9691 - val_loss: 0.9619 - val_accuracy: 0.7756\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0693 - accuracy: 0.9730 - val_loss: 1.0411 - val_accuracy: 0.7651\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0605 - accuracy: 0.9749 - val_loss: 1.1020 - val_accuracy: 0.7690\n"
          ]
        }
      ],
      "source": [
        "# Create 1D CNN layer to model sequences\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = tf.keras.layers.Conv1D(filters=64,\n",
        "                           kernel_size=5,\n",
        "                           padding=\"valid\",\n",
        "                           activation=\"relu\")(x)\n",
        "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_5 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_5_history = model_5.fit(train_sentences,train_labels,\n",
        "                              validation_data=(val_sentences,val_labels),\n",
        "                              epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 969us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2.1116717e-01],\n",
              "       [6.1397952e-01],\n",
              "       [9.9995577e-01],\n",
              "       [4.7976971e-02],\n",
              "       [2.9762123e-07],\n",
              "       [9.9739838e-01],\n",
              "       [9.9166274e-01],\n",
              "       [9.9978369e-01],\n",
              "       [9.9999893e-01],\n",
              "       [7.4387318e-01]], dtype=float32)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_pred_probs = model_5.predict(val_sentences)\n",
        "model_5_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
        "model_5_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.9028871391076,\n",
              " 'precision': 0.7697342660172047,\n",
              " 'recall': 0.7690288713910761,\n",
              " 'f1': 0.7674269881907728}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_results = calculate_results(val_labels,model_5_preds)\n",
        "model_5_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 6 - Tensorflow Hub pretrained models\n",
        "USE-extractor - universal sentence encoder (USE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-0.01157027  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n",
            "  0.02680985  0.05589838 -0.01068729 -0.00597292  0.00639323 -0.0181952\n",
            "  0.00030814  0.09105888  0.05874645 -0.03180628  0.01512474 -0.05162929\n",
            "  0.00991367 -0.06865346 -0.04209305  0.0267898   0.03011008  0.00321069\n",
            " -0.00337971 -0.04787356  0.02266719 -0.00985925 -0.04063613 -0.01292093\n",
            " -0.04666384  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014441\n",
            "  0.02871508  0.04947684 -0.00633978 -0.08960193  0.02807117 -0.00808362\n",
            " -0.01360601  0.0599865  -0.10361787 -0.05195374  0.00232955 -0.0233253\n",
            " -0.03758105  0.03327729], shape=(50,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "embed_sample = embed([sample_sentence,\n",
        "                      \"When you can the unvidersal sentence encider on a sentence, it turns into into numbers\"])\n",
        "print(embed_sample[0][:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
              "array([[-0.01157027,  0.0248591 ,  0.02878048, ..., -0.00186125,\n",
              "         0.02315824, -0.01485021],\n",
              "       [ 0.06090042, -0.08857404,  0.01838204, ..., -0.00653699,\n",
              "         0.03106263,  0.0370101 ]], dtype=float32)>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the above USE\"\" embedding layer for a normal type of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a keras layer using the USE\"\" pretrained layer from tensorflow hub\n",
        "sentence_encoder_layer = hub.KerasLayer(hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\"),\n",
        "                                        input_shape=[],\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow_hub.keras_layer.KerasLayer at 0x7f02ddceff10>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_encoder_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 3s 8ms/step - loss: 0.6501 - accuracy: 0.7362 - val_loss: 0.6150 - val_accuracy: 0.7664\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.5824 - accuracy: 0.7901 - val_loss: 0.5650 - val_accuracy: 0.7782\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.7933 - val_loss: 0.5329 - val_accuracy: 0.7808\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.5100 - accuracy: 0.7990 - val_loss: 0.5115 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.4897 - accuracy: 0.8013 - val_loss: 0.4969 - val_accuracy: 0.7874\n"
          ]
        }
      ],
      "source": [
        "# Create  sequential layer\n",
        "model_6 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_6_history = model_6.fit(train_sentences,train_labels,\n",
        "                               validation_data=(val_sentences,val_labels),\n",
        "                               epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.35795432],\n",
              "       [0.6764395 ],\n",
              "       [0.85277396],\n",
              "       [0.34947234],\n",
              "       [0.64873725],\n",
              "       [0.7345567 ],\n",
              "       [0.8271819 ],\n",
              "       [0.8393318 ],\n",
              "       [0.7466447 ],\n",
              "       [0.2045286 ]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_pred_probs=model_6.predict(val_sentences)\n",
        "model_6_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_preds=tf.squeeze(tf.round(model_6_pred_probs))\n",
        "model_6_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.74015748031496,\n",
              " 'precision': 0.7878968015414602,\n",
              " 'recall': 0.7874015748031497,\n",
              " 'f1': 0.7862340407224946}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_results = calculate_results(val_labels,model_6_preds)\n",
        "model_6_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.4888 - accuracy: 0.7767 - val_loss: 0.4380 - val_accuracy: 0.8045\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.4022 - accuracy: 0.8218 - val_loss: 0.4231 - val_accuracy: 0.8202\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3812 - accuracy: 0.8378 - val_loss: 0.4229 - val_accuracy: 0.8163\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3612 - accuracy: 0.8475 - val_loss: 0.4174 - val_accuracy: 0.8189\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3356 - accuracy: 0.8615 - val_loss: 0.4196 - val_accuracy: 0.8241\n",
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.12460937]\n",
            " [0.7986058 ]\n",
            " [0.99816906]\n",
            " [0.1912126 ]\n",
            " [0.75334513]\n",
            " [0.8186079 ]\n",
            " [0.984877  ]\n",
            " [0.99297863]\n",
            " [0.9755053 ]\n",
            " [0.1012126 ]]\n",
            "tf.Tensor([0. 1. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 82.41469816272966, 'precision': 0.8256511205330104, 'recall': 0.8241469816272966, 'f1': 0.8230145947665632}\n"
          ]
        }
      ],
      "source": [
        "# Challenge, beat the baseline\n",
        "model_6_exp = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_6_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_6_exp_history = model_6_exp.fit(train_sentences,train_labels,\n",
        "                                      epochs=5,\n",
        "                                      validation_data=(val_sentences,val_labels))\n",
        "\n",
        "model_6_exp_pred_probs = model_6_exp.predict(val_sentences)\n",
        "print(model_6_exp_pred_probs[:10])\n",
        "\n",
        "model_6_exp_preds = tf.squeeze(tf.round(model_6_exp_pred_probs))\n",
        "print(model_6_exp_preds[:10])\n",
        "\n",
        "model_6_exp_results = calculate_results(val_labels, model_6_exp_preds)\n",
        "print(model_6_exp_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 7 : TF Hub Pretrained USE but with 10% of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(761, 761)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
        "train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
        "train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
        "len(train_sentences_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    413\n",
              "1    348\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the number of targets in the dataset\n",
        "train_10_percent[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df_shuffled[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 1s 21ms/step - loss: 0.6745 - accuracy: 0.6347 - val_loss: 0.6350 - val_accuracy: 0.7913\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5776 - accuracy: 0.8068 - val_loss: 0.4946 - val_accuracy: 0.8241\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.4486 - accuracy: 0.8187 - val_loss: 0.3936 - val_accuracy: 0.8425\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.3795 - accuracy: 0.8463 - val_loss: 0.3396 - val_accuracy: 0.8609\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.3330 - accuracy: 0.8686 - val_loss: 0.2966 - val_accuracy: 0.8832\n"
          ]
        }
      ],
      "source": [
        "# model 7\n",
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_7_history = model_7.fit(train_sentences_10_percent, train_labels_10_percent,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.09911713],\n",
              "       [0.8881445 ],\n",
              "       [0.9659027 ],\n",
              "       [0.36552706],\n",
              "       [0.9254515 ],\n",
              "       [0.9094441 ],\n",
              "       [0.9597035 ],\n",
              "       [0.98577917],\n",
              "       [0.91345173],\n",
              "       [0.01857546]], dtype=float32)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "model_7_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "model_7_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 88.32020997375328,\n",
              " 'precision': 0.8836591055325493,\n",
              " 'recall': 0.8832020997375328,\n",
              " 'f1': 0.8828731455017578}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_results = calculate_results(val_labels,model_7_preds)\n",
        "model_7_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model 7 did very good compared to model 6, model is same, but only 10% data, why?\n",
        "what could the reason be:\n",
        "1. maybe the way we created 10% data\n",
        "\n",
        "ANS : train_sentences_10_percent has little amount of val_sentences, becuase we took 10% from whole train_df_shuffled variable\n",
        "\n",
        "So, it is already seeing what it will be validating on.\n",
        "\n",
        "This is called data leak problem, some part of validation/testing data leaks into training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(685, 685)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Making correct data allocation - without data leakage\n",
        "train_10_percent_split = int(0.1 * len(train_sentences))    # train_sentences are already in random order, no need to shuffle\n",
        "train_sentence_10_percent = train_sentences[:train_10_percent_split]\n",
        "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
        "len(train_sentence_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.6678 - accuracy: 0.5912 - val_loss: 0.6525 - val_accuracy: 0.5472\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5857 - accuracy: 0.6832 - val_loss: 0.5773 - val_accuracy: 0.7520\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4762 - accuracy: 0.8307 - val_loss: 0.5131 - val_accuracy: 0.7808\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3910 - accuracy: 0.8394 - val_loss: 0.5049 - val_accuracy: 0.7822\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3346 - accuracy: 0.8599 - val_loss: 0.5143 - val_accuracy: 0.7756\n"
          ]
        }
      ],
      "source": [
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "mdoel_7_history_split = model_7.fit(train_sentence_10_percent, train_labels_10_percent,\n",
        "                                    epochs=5,\n",
        "                                    validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.0608828 ]\n",
            " [0.5566165 ]\n",
            " [0.94135475]\n",
            " [0.42704904]\n",
            " [0.67850745]\n",
            " [0.6879722 ]\n",
            " [0.9303501 ]\n",
            " [0.859324  ]\n",
            " [0.92495406]\n",
            " [0.08896577]]\n",
            "tf.Tensor([0. 1. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 77.55905511811024, 'precision': 0.7759863909628747, 'recall': 0.7755905511811023, 'f1': 0.7743062301518678}\n"
          ]
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "print(model_7_pred_probs[:10])\n",
        "\n",
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "print(model_7_preds[:10])\n",
        "\n",
        "model_7_results = calculate_results(val_labels, model_7_preds)\n",
        "print(model_7_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
