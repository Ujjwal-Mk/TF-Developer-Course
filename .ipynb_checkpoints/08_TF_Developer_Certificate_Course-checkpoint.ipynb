{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rj7sBkxRK6G",
        "outputId": "1e663cc2-0b3a-43f0-d474-9f7e1e112a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67y0phrRQnz",
        "outputId": "81d04c0b-1f69-419f-f579-b8cd75387b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 'helper_functions.py' already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "  !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "  print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n",
        "\n",
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5L-H4otpgvS"
      },
      "source": [
        "We will be using RNN to train textual information\n",
        "\n",
        "**Basic Structure of RNN**\n",
        "* Input layer\n",
        "* text_vectorizer(input)\n",
        "* embedding(x)\n",
        "* LSTM -> activation = tanh\n",
        "* output_Dense -> activation = sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0eqNb9Nsyzi"
      },
      "source": [
        "### Get the text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Fp9BY2pkCZ",
        "outputId": "68fbdfce-f7fa-4d30-89d2-7ee2bfeeac45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
        "    !wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "    unzip_data(\"nlp_getting_started.zip\")\n",
        "else:\n",
        "  print(\"[INFO] already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzD1DRKtRlJ"
      },
      "source": [
        "### Visualizing a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qyWmNLXTtC_x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BoTo82yJt4no",
        "outputId": "913f97ff-16a7-4f96-c138-4b2126474a9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5ImBTHVft6K2",
        "outputId": "0b19d104-d524-4206-9c13-486aa55e5c34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>7934</td>\n",
              "      <td>rainstorm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@Calum5SOS you look like you got caught in a r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>2538</td>\n",
              "      <td>collision</td>\n",
              "      <td>NaN</td>\n",
              "      <td>my favorite lady came to our volunteer meeting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>2611</td>\n",
              "      <td>crashed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@brianroemmele UX fail of EMV - people want to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6810</th>\n",
              "      <td>9756</td>\n",
              "      <td>tragedy</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Can't find my ariana grande shirt  this is a f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>6254</td>\n",
              "      <td>hijacking</td>\n",
              "      <td>Athens,Greece</td>\n",
              "      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                    NaN   \n",
              "2227  3185       deluge                    NaN   \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                    NaN   \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "5559  7934    rainstorm                    NaN   \n",
              "1765  2538    collision                    NaN   \n",
              "1817  2611      crashed                    NaN   \n",
              "6810  9756      tragedy        Los Angeles, CA   \n",
              "4398  6254    hijacking          Athens,Greece   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  \n",
              "5559  @Calum5SOS you look like you got caught in a r...       0  \n",
              "1765  my favorite lady came to our volunteer meeting...       1  \n",
              "1817  @brianroemmele UX fail of EMV - people want to...       1  \n",
              "6810  Can't find my ariana grande shirt  this is a f...       0  \n",
              "4398  The Murderous Story Of AmericaÛªs First Hijac...       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Shuffle to train data\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "train_df_shuffled.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2wbB2XBBuUsd",
        "outputId": "5d7aa573-919e-4194-a637-848374c73253"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're shaking...It's an earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They'd probably still show more life than Arse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey! How are you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a nice hat?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fuck off!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
              "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
              "7  22     NaN      NaN                                  Hey! How are you?\n",
              "8  27     NaN      NaN                                   What a nice hat?\n",
              "9  29     NaN      NaN                                          Fuck off!"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_data\n",
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNWm50cubJ8",
        "outputId": "660bd2ae-b113-448f-d7fd-7a4a79e2cd2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#How many examples in each class\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA79OdN4umTy",
        "outputId": "d1f9094c-38b4-494b-e75f-d7ee32698c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4245\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "#FOXDebateQuestions:  To what degree has Obama's efforts to institute Sharia Law exacerbated the California wild fires?\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "#hot#teen#nsfw#porn#milf: Oiled Up Ass Hole Is Destroyed With King Size Cock Closeup Sex Clip http://t.co/faoGxkwdpG\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "Car engulfed in flames backs up traffic at ParleyÛªs Summit http://t.co/RmucfjCaZr\n",
            "\n",
            "---\n",
            "Target: 0, (not a real disaster)\n",
            "Text:\n",
            "@AcaciaPenn I'll start a big ass riot  send me to jail today mfs shidddd ??\n",
            "\n",
            "---\n",
            "Target: 1, (real disaster)\n",
            "Text:\n",
            "POV video captures violent landing at Amsterdam Airport Schiphol during a storm - Daily Mail http://t.co/seShqN5DSK #Amsterdam #News\n",
            "\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# visualise some random training examples\n",
        "import random\n",
        "random_index = random.randint(0,len(train_df)-5)\n",
        "print(random_index)\n",
        "for row in train_df_shuffled[['text','target']][random_index:random_index+5].itertuples():\n",
        "  index,text,target = row\n",
        "  print(f\"Target: {target}, {'(real disaster)' if target > 0 else '(not a real disaster)'}\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRuczOy1Hze",
        "outputId": "7fd6c85e-4713-4d69-e837-61fd589df245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_df_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ilKeV5jwa0E"
      },
      "outputs": [],
      "source": [
        "#Splitting the data to create validation -> (train + validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2LtkKK08Rd",
        "outputId": "7109197e-f115-4f06-8be1-d9913d133cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check length\n",
        "len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZ7B-jT1EHj",
        "outputId": "66494cf4-27da-4c56-e9ee-5a9272a8b1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object),\n",
              " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check few sample\n",
        "train_sentences[:10],train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfc38t53GjO"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "1. tokenization -> can get too big as the number of text sample increases\n",
        "2. Embedding -> richer representation of relationship be between tokens, deeper the realtion between words, more the number/embedding for that token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yru6-5TJ1U17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 8.00 GB\n",
            "maxCacheSize: 2.67 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                                    split=\"whitespace\", # how to split tokens\n",
        "                                                    ngrams=None, # create groups of n-words?\n",
        "                                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_length,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "L B #Entertainment lot of 8 #BruceWillis MOVIES #DVD DIE HARD 1 2 12 MONKEYS ARMAGEDDON SIXTH #eBay #Auction http://t.co/CxDJApzXMP      \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[3696,  964, 2124,  505,    6,  589,    1, 2037, 2482,  686,  892,\n",
              "         198,   70,  719, 5096]])>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
          ]
        }
      ],
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x28396cd60>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\") \n",
        "\n",
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Episcopal priests on road trip with interracial family shares harrowing story of police harassment http://t.co/RG4JIsHyBs via @dailykos\n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
              "          0.03332629,  0.02803668],\n",
              "        [ 0.04241042, -0.02964536, -0.0423249 , ...,  0.00976134,\n",
              "          0.04075502,  0.03627021],\n",
              "        [-0.02399485,  0.01468222,  0.00041829, ...,  0.02498427,\n",
              "         -0.02674054, -0.00808267],\n",
              "        ...,\n",
              "        [ 0.02169076,  0.02180234,  0.03649724, ..., -0.00753174,\n",
              "         -0.03365277, -0.02953072],\n",
              "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
              "          0.03332629,  0.02803668],\n",
              "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
              "          0.03332629,  0.02803668]]], dtype=float32)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([ 0.03977952, -0.03782602, -0.03646283, -0.02449075, -0.00015752,\n",
              "        0.02220254,  0.00162981,  0.00603487,  0.0085157 , -0.02620113,\n",
              "        0.04101599,  0.03715892,  0.02397566,  0.00281113, -0.02704906,\n",
              "       -0.04870148,  0.01457943,  0.0059551 , -0.02334484,  0.03581132,\n",
              "        0.04377897,  0.04186075,  0.03245703, -0.045092  ,  0.04260418,\n",
              "        0.03398135, -0.01812425, -0.03539513,  0.02954218,  0.02556742,\n",
              "       -0.03345481,  0.04272738, -0.00798845, -0.0406163 , -0.00644834,\n",
              "        0.00232404,  0.01703629,  0.03645121, -0.02622857,  0.03498118,\n",
              "       -0.03059715,  0.02576998, -0.04221511,  0.02654583, -0.02192564,\n",
              "       -0.0346157 ,  0.00075326,  0.01427345,  0.01027539, -0.04311384,\n",
              "       -0.03973336, -0.00966626,  0.01032177, -0.04011822, -0.018892  ,\n",
              "       -0.01233201,  0.02721632, -0.01232889, -0.02504088, -0.04715574,\n",
              "        0.00558523, -0.00801403,  0.03058865, -0.01923352, -0.04175536,\n",
              "       -0.03654208, -0.04277095, -0.0033918 , -0.04226271,  0.0240727 ,\n",
              "        0.01274442, -0.00268712,  0.03158386,  0.04823284,  0.02940297,\n",
              "        0.00636031,  0.01719667,  0.03907609, -0.0249153 , -0.00834242,\n",
              "        0.03881217, -0.04461795,  0.03740455, -0.02412283, -0.01538421,\n",
              "        0.03984089, -0.04073881,  0.0317078 , -0.04241145, -0.03435747,\n",
              "        0.03021734,  0.0443267 ,  0.00424304, -0.01204874, -0.00213076,\n",
              "       -0.02668054,  0.0461436 , -0.04644201,  0.02519932, -0.04503984,\n",
              "       -0.03163396, -0.02846084,  0.01276297,  0.02938429,  0.03437588,\n",
              "        0.00973482, -0.01860742,  0.03510927, -0.04996962,  0.01983938,\n",
              "        0.00642185,  0.00372197,  0.01576127,  0.01591486, -0.04195998,\n",
              "        0.02777341,  0.00619875, -0.02269079, -0.00224097,  0.0289592 ,\n",
              "        0.04377018, -0.03377642,  0.00907839, -0.01134553, -0.02910458,\n",
              "        0.00236253,  0.03332629,  0.02803668], dtype=float32)>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelling a text dataset\n",
        "More specifically, we'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model 0\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our baseline model achieves an accuracy of: 79.27%\n"
          ]
        }
      ],
      "source": [
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_functions import calculate_results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 1: A simple dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 128)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create directory to save TensorBoard logs\n",
        "SAVE_DIR = \"model_logs\"\n",
        "\n",
        "# Build model with the Functional API\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into numbers\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
        "# Compile model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/simple_dense_model/20230820-020724\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 23ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 4s 16ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
        "                                                                     experiment_name=\"simple_dense_model\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 7ms/step - loss: 0.4767 - accuracy: 0.7874\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.47668465971946716, 0.787401556968689]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.4048821 ],\n",
              "       [0.7443312 ],\n",
              "       [0.997895  ],\n",
              "       [0.1089    ],\n",
              "       [0.1114353 ],\n",
              "       [0.93556094],\n",
              "       [0.9134595 ],\n",
              "       [0.9925345 ],\n",
              "       [0.97156817],\n",
              "       [0.26570344]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_preds=tf.squeeze(tf.round(model_1_pred_probs))\n",
        "model_1_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.74015748031496,\n",
              " 'precision': 0.7914920592553047,\n",
              " 'recall': 0.7874015748031497,\n",
              " 'f1': 0.7846966492209201}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1_results=calculate_results(val_labels,model_1_preds)\n",
        "model_1_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recurrent Neural Network\n",
        "* RNN's are useful for sequence data\n",
        "* The premise of a recurrent neural network is to use the representation of previous input to aid the representation of a later input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 : LSTM -> Long short term memory\n",
        "\n",
        "Input(text) -> Toeknize -> Embedding -> Layers(RNN/Dense) -> Output(label probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 15, 128)\n",
            "(None, 64)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 15, 64)            49408     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 15, 64)            33024     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,395,521\n",
            "Trainable params: 1,395,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Set random seed and create embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_2_embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.LSTM(64)(x) # return vector for whole sequence\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20230820-020758\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 21s 62ms/step - loss: 0.5075 - accuracy: 0.7505 - val_loss: 0.4592 - val_accuracy: 0.7822\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 12s 54ms/step - loss: 0.3249 - accuracy: 0.8717 - val_loss: 0.5095 - val_accuracy: 0.7808\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 7s 34ms/step - loss: 0.2294 - accuracy: 0.9150 - val_loss: 0.5492 - val_accuracy: 0.7585\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 7s 32ms/step - loss: 0.1739 - accuracy: 0.9384 - val_loss: 0.6230 - val_accuracy: 0.7756\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 7s 32ms/step - loss: 0.1261 - accuracy: 0.9521 - val_loss: 0.8408 - val_accuracy: 0.7664\n"
          ]
        }
      ],
      "source": [
        "# Fit model\n",
        "model_2_history = model_2.fit(train_sentences, train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
        "                                                                     \"LSTM\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 1s 16ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.01034616],\n",
              "       [0.60270125],\n",
              "       [0.9994808 ],\n",
              "       [0.07978331],\n",
              "       [0.00589019],\n",
              "       [0.99949956],\n",
              "       [0.48746157],\n",
              "       [0.9996024 ],\n",
              "       [0.9995084 ],\n",
              "       [0.09803279]], dtype=float32)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 0., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert preds to laels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.64041994750657,\n",
              " 'precision': 0.7763842038829116,\n",
              " 'recall': 0.7664041994750657,\n",
              " 'f1': 0.7609273470439181}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2_results = calculate_results(val_labels, model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 3: GRU\n",
        "\n",
        "1. Gated recurrent unit is effective and popular\n",
        "2. GRU cell has similar features to LSTM cell but has less paramenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 9s 30ms/step - loss: 0.2353 - accuracy: 0.9034 - val_loss: 0.5313 - val_accuracy: 0.7730\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.1648 - accuracy: 0.9394 - val_loss: 0.5805 - val_accuracy: 0.7769\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.1319 - accuracy: 0.9524 - val_loss: 0.7493 - val_accuracy: 0.7795\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 5s 21ms/step - loss: 0.1144 - accuracy: 0.9583 - val_loss: 0.7265 - val_accuracy: 0.7835\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.0956 - accuracy: 0.9654 - val_loss: 0.7408 - val_accuracy: 0.7795\n"
          ]
        }
      ],
      "source": [
        "# Build GRU RNN\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "# x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "# x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "x = tf.keras.layers.GRU(64)(x)\n",
        "# x = tf.keras.layers.Dense(64,activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                        train_labels,\n",
        "                        epochs=5,\n",
        "                        validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.12145904],\n",
              "       [0.81884205],\n",
              "       [0.999258  ],\n",
              "       [0.03835925],\n",
              "       [0.00208007],\n",
              "       [0.988282  ],\n",
              "       [0.77536994],\n",
              "       [0.9994541 ],\n",
              "       [0.99903274],\n",
              "       [0.6994909 ]], dtype=float32)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert model 3 pred_probs to labels\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 77.95275590551181,\n",
              " 'precision': 0.780644760213944,\n",
              " 'recall': 0.7795275590551181,\n",
              " 'f1': 0.7778858484546237}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_3_results = calculate_results(val_labels,model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model4 - Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 17s 61ms/step - loss: 0.5051 - accuracy: 0.7524 - val_loss: 0.4567 - val_accuracy: 0.7822\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 10s 46ms/step - loss: 0.3128 - accuracy: 0.8748 - val_loss: 0.5210 - val_accuracy: 0.7664\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 10s 45ms/step - loss: 0.2091 - accuracy: 0.9204 - val_loss: 0.5622 - val_accuracy: 0.7533\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 9s 44ms/step - loss: 0.1452 - accuracy: 0.9512 - val_loss: 0.7059 - val_accuracy: 0.7612\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 9s 44ms/step - loss: 0.1041 - accuracy: 0.9679 - val_loss: 0.7163 - val_accuracy: 0.7690\n"
          ]
        }
      ],
      "source": [
        "# Trying out on my own\n",
        "text_vectorizer_exp = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                      output_sequence_length=15)\n",
        "text_vectorizer_exp.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer_exp(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4_exp = tf.keras.Model(inputs,outputs)\n",
        "model_4_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_4_history_exp = model_4_exp.fit(train_sentences,\n",
        "                                      train_labels,\n",
        "                                      validation_data=(val_sentences,val_labels),\n",
        "                                      epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normal RNN's go from left to right\n",
        "\n",
        "Bi-directional go from right to left as well as left to right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 11s 40ms/step - loss: 0.5070 - accuracy: 0.7489 - val_loss: 0.4568 - val_accuracy: 0.7808\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 6s 28ms/step - loss: 0.3117 - accuracy: 0.8726 - val_loss: 0.5310 - val_accuracy: 0.7703\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 6s 30ms/step - loss: 0.2102 - accuracy: 0.9183 - val_loss: 0.5691 - val_accuracy: 0.7730\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 6s 29ms/step - loss: 0.1423 - accuracy: 0.9530 - val_loss: 0.6416 - val_accuracy: 0.7743\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 6s 29ms/step - loss: 0.1011 - accuracy: 0.9641 - val_loss: 0.6742 - val_accuracy: 0.7769\n"
          ]
        }
      ],
      "source": [
        "# model 4\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=10000,\n",
        "                                                    output_sequence_length=15)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = tf.keras.layers.Embedding(input_dim=10000,\n",
        "                              output_dim=128,\n",
        "                              input_length=15)(x)\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_4_history = model_4.fit(train_sentences,train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 1s 15ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.0241131 ],\n",
              "       [0.88096327],\n",
              "       [0.99908984],\n",
              "       [0.17479654],\n",
              "       [0.00485721],\n",
              "       [0.9949734 ],\n",
              "       [0.9446198 ],\n",
              "       [0.9996538 ],\n",
              "       [0.9995653 ],\n",
              "       [0.23299517]], dtype=float32)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 77.69028871391076,\n",
              " 'precision': 0.7769803060533722,\n",
              " 'recall': 0.7769028871391076,\n",
              " 'f1': 0.7758759837011532}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_4_results = calculate_results(val_labels,model_4_preds)\n",
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n",
        "conv_1d = tf.keras.layers.Conv1D(32,\n",
        "                                 5,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding=\"valid\")\n",
        "conv_1d_output = conv_1d(embedding_test)\n",
        "\n",
        "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output)\n",
        "\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.03134985, -0.06222165, -0.02360567, ...,  0.02404287,\n",
              "         -0.05508155, -0.02844757],\n",
              "        [-0.05757931,  0.05949935, -0.00868816, ...,  0.02166429,\n",
              "         -0.03304061,  0.06836226],\n",
              "        [-0.05111449, -0.02627705, -0.03455151, ..., -0.01190107,\n",
              "          0.01225715,  0.02233405],\n",
              "        ...,\n",
              "        [-0.01113634,  0.01505618, -0.0324578 , ..., -0.02053399,\n",
              "         -0.01800286,  0.00564639],\n",
              "        [-0.01113634,  0.01505618, -0.0324578 , ..., -0.02053399,\n",
              "         -0.01800286,  0.00564639],\n",
              "        [-0.01113634,  0.01505618, -0.0324578 , ..., -0.02053399,\n",
              "         -0.01800286,  0.00564639]]], dtype=float32)>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
              "array([[[0.03297443, 0.        , 0.        , 0.        , 0.04464962,\n",
              "         0.        , 0.        , 0.        , 0.08062426, 0.        ,\n",
              "         0.05646043, 0.04143378, 0.01133053, 0.        , 0.        ,\n",
              "         0.02713361, 0.        , 0.0809781 , 0.        , 0.06335934,\n",
              "         0.        , 0.        , 0.02930275, 0.        , 0.0580885 ,\n",
              "         0.00150857, 0.08014834, 0.        , 0.05059809, 0.        ,\n",
              "         0.01414267, 0.01151966],\n",
              "        [0.00729813, 0.01207459, 0.        , 0.04123987, 0.        ,\n",
              "         0.00291226, 0.        , 0.        , 0.00349986, 0.        ,\n",
              "         0.        , 0.02776924, 0.05610539, 0.01356795, 0.        ,\n",
              "         0.06123743, 0.        , 0.        , 0.02227192, 0.0112344 ,\n",
              "         0.07152978, 0.        , 0.01045766, 0.        , 0.03610864,\n",
              "         0.04951687, 0.        , 0.        , 0.08513662, 0.        ,\n",
              "         0.04155697, 0.        ],\n",
              "        [0.02039959, 0.        , 0.00660283, 0.04463048, 0.03102958,\n",
              "         0.02225268, 0.04206178, 0.        , 0.09976071, 0.        ,\n",
              "         0.01354013, 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.03357195, 0.02284411,\n",
              "         0.04761577, 0.        , 0.        , 0.        , 0.08961818,\n",
              "         0.        , 0.        , 0.        , 0.03092771, 0.02457768,\n",
              "         0.04692761, 0.01232958],\n",
              "        [0.00721456, 0.        , 0.        , 0.        , 0.1412369 ,\n",
              "         0.01085687, 0.        , 0.        , 0.05085006, 0.        ,\n",
              "         0.        , 0.02446802, 0.        , 0.00178429, 0.        ,\n",
              "         0.        , 0.01126975, 0.        , 0.        , 0.00609108,\n",
              "         0.03540119, 0.03847308, 0.        , 0.01215534, 0.01543593,\n",
              "         0.05031298, 0.        , 0.02691743, 0.        , 0.        ,\n",
              "         0.00301653, 0.        ],\n",
              "        [0.02102274, 0.        , 0.        , 0.00501447, 0.05138396,\n",
              "         0.        , 0.00451975, 0.        , 0.07020561, 0.        ,\n",
              "         0.        , 0.00857347, 0.        , 0.00761564, 0.        ,\n",
              "         0.        , 0.        , 0.02072784, 0.        , 0.01140874,\n",
              "         0.03114459, 0.        , 0.        , 0.01077716, 0.0197811 ,\n",
              "         0.        , 0.00431472, 0.        , 0.05031288, 0.        ,\n",
              "         0.03402563, 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ],\n",
              "        [0.02622849, 0.        , 0.        , 0.01051132, 0.03973497,\n",
              "         0.        , 0.        , 0.        , 0.0710917 , 0.        ,\n",
              "         0.        , 0.04586256, 0.        , 0.02457473, 0.        ,\n",
              "         0.        , 0.02438542, 0.01693739, 0.0145282 , 0.04340753,\n",
              "         0.02216071, 0.        , 0.        , 0.00373737, 0.01067499,\n",
              "         0.        , 0.        , 0.        , 0.04231416, 0.        ,\n",
              "         0.00572   , 0.        ]]], dtype=float32)>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_1d_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              "array([[0.03297443, 0.01207459, 0.00660283, 0.04463048, 0.1412369 ,\n",
              "        0.02225268, 0.04206178, 0.        , 0.09976071, 0.        ,\n",
              "        0.05646043, 0.04586256, 0.05610539, 0.02457473, 0.        ,\n",
              "        0.06123743, 0.02438542, 0.0809781 , 0.03357195, 0.06335934,\n",
              "        0.07152978, 0.03847308, 0.02930275, 0.01215534, 0.08961818,\n",
              "        0.05031298, 0.08014834, 0.02691743, 0.08513662, 0.02457768,\n",
              "        0.04692761, 0.01232958]], dtype=float32)>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_pool_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 27ms/step - loss: 0.1551 - accuracy: 0.9466 - val_loss: 0.7516 - val_accuracy: 0.7808\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.1055 - accuracy: 0.9604 - val_loss: 0.8920 - val_accuracy: 0.7756\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.0829 - accuracy: 0.9682 - val_loss: 0.9723 - val_accuracy: 0.7743\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.0713 - accuracy: 0.9723 - val_loss: 1.0503 - val_accuracy: 0.7638\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0611 - accuracy: 0.9759 - val_loss: 1.1413 - val_accuracy: 0.7690\n"
          ]
        }
      ],
      "source": [
        "# Create 1D CNN layer to model sequences\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = tf.keras.layers.Conv1D(filters=64,\n",
        "                           kernel_size=5,\n",
        "                           padding=\"valid\",\n",
        "                           activation=\"relu\")(x)\n",
        "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)(x)\n",
        "\n",
        "model_5 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_5_history = model_5.fit(train_sentences,train_labels,\n",
        "                              validation_data=(val_sentences,val_labels),\n",
        "                              epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2.0180912e-01],\n",
              "       [6.0437667e-01],\n",
              "       [9.9995875e-01],\n",
              "       [4.5819260e-02],\n",
              "       [1.7597493e-07],\n",
              "       [9.9612927e-01],\n",
              "       [9.7530907e-01],\n",
              "       [9.9988127e-01],\n",
              "       [9.9999905e-01],\n",
              "       [7.9330397e-01]], dtype=float32)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_pred_probs = model_5.predict(val_sentences)\n",
        "model_5_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
        "model_5_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 76.9028871391076,\n",
              " 'precision': 0.7701356893081864,\n",
              " 'recall': 0.7690288713910761,\n",
              " 'f1': 0.7671875324347506}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_5_results = calculate_results(val_labels,model_5_preds)\n",
        "model_5_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model 6 - Tensorflow Hub pretrained models\n",
        "USE-extractor - universal sentence encoder (USE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-0.01157026  0.0248591   0.02878049 -0.01271502  0.03971538  0.08827759\n",
            "  0.02680982  0.05589838 -0.01068733 -0.00597293  0.00639323 -0.01819518\n",
            "  0.00030815  0.09105889  0.05874642 -0.03180628  0.01512474 -0.05162928\n",
            "  0.00991367 -0.06865344 -0.04209306  0.02678977  0.0301101   0.00321067\n",
            " -0.00337969 -0.04787359  0.02266718 -0.00985926 -0.04063614 -0.01292093\n",
            " -0.04666385  0.05630299 -0.03949255  0.00517686  0.02495827 -0.07014441\n",
            "  0.02871507  0.04947682 -0.00633977 -0.0896019   0.02807121 -0.00808364\n",
            " -0.01360602  0.0599865  -0.10361786 -0.05195374  0.00232954 -0.0233253\n",
            " -0.03758108  0.03327726], shape=(50,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "embed_sample = embed([sample_sentence,\n",
        "                      \"When you can the unvidersal sentence encider on a sentence, it turns into into numbers\"])\n",
        "print(embed_sample[0][:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
              "array([[-0.01157026,  0.0248591 ,  0.02878049, ..., -0.00186124,\n",
              "         0.02315822, -0.01485022],\n",
              "       [ 0.06090041, -0.08857401,  0.01838204, ..., -0.00653699,\n",
              "         0.03106263,  0.03701009]], dtype=float32)>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the above USE\"\" embedding layer for a normal type of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a keras layer using the USE\"\" pretrained layer from tensorflow hub\n",
        "sentence_encoder_layer = hub.KerasLayer(hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\"),\n",
        "                                        input_shape=[],\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow_hub.keras_layer.KerasLayer at 0x3ea156e80>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_encoder_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 67/215 [========>.....................] - ETA: 15s - loss: 0.6832 - accuracy: 0.6208"
          ]
        }
      ],
      "source": [
        "# Create  sequential layer\n",
        "model_6 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "model_6_history = model_6.fit(train_sentences,train_labels,\n",
        "                               validation_data=(val_sentences,val_labels),\n",
        "                               epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.37072343],\n",
              "       [0.67399824],\n",
              "       [0.849519  ],\n",
              "       [0.3633444 ],\n",
              "       [0.6382592 ],\n",
              "       [0.7322892 ],\n",
              "       [0.8246749 ],\n",
              "       [0.84339666],\n",
              "       [0.7467693 ],\n",
              "       [0.1936599 ]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_pred_probs=model_6.predict(val_sentences)\n",
        "model_6_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_preds=tf.squeeze(tf.round(model_6_pred_probs))\n",
        "model_6_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 78.87139107611549,\n",
              " 'precision': 0.7896925793067333,\n",
              " 'recall': 0.7887139107611548,\n",
              " 'f1': 0.7873013461184764}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_6_results = calculate_results(val_labels,model_6_preds)\n",
        "model_6_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 79.26509186351706,\n",
              " 'precision': 0.8111390004213173,\n",
              " 'recall': 0.7926509186351706,\n",
              " 'f1': 0.7862189758049549}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 2s 7ms/step - loss: 0.4821 - accuracy: 0.7857 - val_loss: 0.4378 - val_accuracy: 0.8005\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.4011 - accuracy: 0.8222 - val_loss: 0.4234 - val_accuracy: 0.8163\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8345 - val_loss: 0.4219 - val_accuracy: 0.8163\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3581 - accuracy: 0.8483 - val_loss: 0.4210 - val_accuracy: 0.8202\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8599 - val_loss: 0.4230 - val_accuracy: 0.8215\n",
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.16142817]\n",
            " [0.79830945]\n",
            " [0.99721247]\n",
            " [0.16722518]\n",
            " [0.6336406 ]\n",
            " [0.86117154]\n",
            " [0.9954034 ]\n",
            " [0.9934882 ]\n",
            " [0.9897069 ]\n",
            " [0.06630458]]\n",
            "tf.Tensor([0. 1. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 82.1522309711286, 'precision': 0.822740944702772, 'recall': 0.821522309711286, 'f1': 0.8204589074026987}\n"
          ]
        }
      ],
      "source": [
        "# Challenge, beat the baseline\n",
        "model_6_exp = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_6_exp.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=['accuracy'])\n",
        "model_6_exp_history = model_6_exp.fit(train_sentences,train_labels,\n",
        "                                      epochs=5,\n",
        "                                      validation_data=(val_sentences,val_labels))\n",
        "\n",
        "model_6_exp_pred_probs = model_6_exp.predict(val_sentences)\n",
        "print(model_6_exp_pred_probs[:10])\n",
        "\n",
        "model_6_exp_preds = tf.squeeze(tf.round(model_6_exp_pred_probs))\n",
        "print(model_6_exp_preds[:10])\n",
        "\n",
        "model_6_exp_results = calculate_results(val_labels, model_6_exp_preds)\n",
        "print(model_6_exp_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 7 : TF Hub Pretrained USE but with 10% of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(761, 761)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
        "train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
        "train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
        "len(train_sentences_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    413\n",
              "1    348\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the number of targets in the dataset\n",
        "train_10_percent[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    4342\n",
              "1    3271\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df_shuffled[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 1s 22ms/step - loss: 0.6699 - accuracy: 0.6413 - val_loss: 0.6290 - val_accuracy: 0.7703\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.5775 - accuracy: 0.8029 - val_loss: 0.5062 - val_accuracy: 0.8215\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.4617 - accuracy: 0.8213 - val_loss: 0.4047 - val_accuracy: 0.8412\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.3875 - accuracy: 0.8423 - val_loss: 0.3459 - val_accuracy: 0.8622\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.3380 - accuracy: 0.8620 - val_loss: 0.3001 - val_accuracy: 0.8780\n"
          ]
        }
      ],
      "source": [
        "# model 7\n",
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_7_history = model_7.fit(train_sentences_10_percent, train_labels_10_percent,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences,val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.09183506],\n",
              "       [0.8766855 ],\n",
              "       [0.955973  ],\n",
              "       [0.22970389],\n",
              "       [0.9057463 ],\n",
              "       [0.8609012 ],\n",
              "       [0.95389783],\n",
              "       [0.97209686],\n",
              "       [0.9284693 ],\n",
              "       [0.01393676]], dtype=float32)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "model_7_pred_probs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "model_7_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 87.79527559055119,\n",
              " 'precision': 0.8787082788740477,\n",
              " 'recall': 0.8779527559055118,\n",
              " 'f1': 0.8775165403027484}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_7_results = calculate_results(val_labels,model_7_preds)\n",
        "model_7_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model 7 did very good compared to model 6, model is same, but only 10% data, why?\n",
        "what could the reason be:\n",
        "1. maybe the way we created 10% data\n",
        "\n",
        "ANS : train_sentences_10_percent has little amount of val_sentences, becuase we took 10% from whole train_df_shuffled variable\n",
        "\n",
        "So, it is already seeing what it will be validating on.\n",
        "\n",
        "This is called data leak problem, some part of validation/testing data leaks into training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(685, 685)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Making correct data allocation - without data leakage\n",
        "train_10_percent_split = int(0.1 * len(train_sentences))    # train_sentences are already in random order, no need to shuffle\n",
        "train_sentence_10_percent = train_sentences[:train_10_percent_split]\n",
        "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
        "len(train_sentence_10_percent), len(train_labels_10_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 108ms/step - loss: 0.6687 - accuracy: 0.6219 - val_loss: 0.6524 - val_accuracy: 0.5997\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.5799 - accuracy: 0.7635 - val_loss: 0.5579 - val_accuracy: 0.7717\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4548 - accuracy: 0.8204 - val_loss: 0.4954 - val_accuracy: 0.7940\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3764 - accuracy: 0.8438 - val_loss: 0.5009 - val_accuracy: 0.7808\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3252 - accuracy: 0.8642 - val_loss: 0.5104 - val_accuracy: 0.7730\n"
          ]
        }
      ],
      "source": [
        "model_7 = tf.keras.models.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(64,activation=tf.keras.activations.relu),\n",
        "    tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "mdoel_7_history_split = model_7.fit(train_sentence_10_percent, train_labels_10_percent,\n",
        "                                    epochs=5,\n",
        "                                    validation_data=(val_sentences,val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 5ms/step\n",
            "[[0.02782756]\n",
            " [0.47674778]\n",
            " [0.9636344 ]\n",
            " [0.4345637 ]\n",
            " [0.64019966]\n",
            " [0.678855  ]\n",
            " [0.9527489 ]\n",
            " [0.8586304 ]\n",
            " [0.9380611 ]\n",
            " [0.09271669]]\n",
            "tf.Tensor([0. 0. 1. 0. 1. 1. 1. 1. 1. 0.], shape=(10,), dtype=float32)\n",
            "{'accuracy': 77.29658792650919, 'precision': 0.7738445106757977, 'recall': 0.7729658792650919, 'f1': 0.7713337273803944}\n"
          ]
        }
      ],
      "source": [
        "model_7_pred_probs = model_7.predict(val_sentences)\n",
        "print(model_7_pred_probs[:10])\n",
        "\n",
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "print(model_7_preds[:10])\n",
        "\n",
        "model_7_results = calculate_results(val_labels, model_7_preds)\n",
        "print(model_7_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the performance of each of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_model_results = pd.DataFrame({\"baseline\":baseline_results,\n",
        "                                  \"simple_dense\":model_1_results,\n",
        "                                  \"lstm\":model_2_results})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
